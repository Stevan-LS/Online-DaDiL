{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SL276123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ot\\backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n",
      "C:\\Users\\SL276123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Tests_ODaDiL import test_dadil, test_odadil, test_forgetting_odadil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # noqa: E402\n",
    "import random  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "\n",
    "from pydil.utils.igmm_modif import IGMM\n",
    "\n",
    "from pydil.ipms.ot_ipms import (  # noqa: E402\n",
    "    JointWassersteinDistance\n",
    ")\n",
    "from pydil.dadil.labeled_dictionary_GMM import LabeledDictionaryGMM\n",
    "from pydil.torch_utils.measures import (  # noqa: E402\n",
    "    UnsupervisedDatasetMeasure,\n",
    "    SupervisedDatasetMeasure\n",
    ")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_datasets = []\n",
    "for i in range(1, 11):\n",
    "    dataset = np.load(f'data/toy_non_linear_100d_dataset_{i}.npy')\n",
    "    list_of_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'C'\n",
    "with open(os.path.join('data', 'mlp_fts_256_target_{}.pkl'.format(target)), 'rb') as f:\n",
    "        dataset = pickle.loads(f.read())\n",
    "\n",
    "Xs, ys = [], []\n",
    "d = None\n",
    "keys = list(dataset.keys())\n",
    "for i in range(len(keys)-1):\n",
    "    features = dataset[keys[i]]['Features']\n",
    "    labels = dataset[keys[i]]['Labels'].argmax(dim=1)\n",
    "    domain = i*np.ones((features.shape[0], 1))\n",
    "    Xs.append(features.float())\n",
    "    ys.append(labels.float())\n",
    "    if d is None:\n",
    "        d = domain\n",
    "    else:\n",
    "        d = np.concatenate([d, domain], axis=0)\n",
    "\n",
    "Xt = dataset[target]['fold 0']['Train']['Features'].float()\n",
    "yt = dataset[target]['fold 0']['Train']['Labels'].float().argmax(dim=1)\n",
    "\n",
    "Xt_test = dataset[target]['fold 0']['Test']['Features'].float()\n",
    "yt_test = dataset[target]['fold 0']['Test']['Labels'].float().argmax(dim=1)\n",
    "d = np.concatenate([d, 2*np.ones((Xt.shape[0], 1))], axis=0)\n",
    "\n",
    "n_domains = int(np.max(d)) + 1\n",
    "n_features = Xt.shape[1]\n",
    "n_classes = int(np.max(yt.numpy())) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "batch_size = 200\n",
    "n_atoms = 3\n",
    "n_classes = 10\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter):\n",
    "    results = {'lin':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'rbf':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'RF':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}}\n",
    "\n",
    "    Q = []\n",
    "    for Xs_k, ys_k in zip(Xs, ys):\n",
    "        Q.append(\n",
    "            SupervisedDatasetMeasure(\n",
    "                features=Xs_k.numpy(),\n",
    "                labels=ys_k.numpy(),\n",
    "                stratify=True,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu'\n",
    "            )\n",
    "        )\n",
    "    Q.append(\n",
    "        UnsupervisedDatasetMeasure(\n",
    "            features=Xt.numpy(),\n",
    "            batch_size=batch_size,\n",
    "            device='cpu'\n",
    "        )\n",
    "    )\n",
    "    criterion = JointWassersteinDistance()\n",
    "    dictionary = LabeledDictionaryGMM(XP=None,\n",
    "                            YP=None,\n",
    "                            A=None,\n",
    "                            n_samples=n_samples,\n",
    "                            n_dim=n_features,\n",
    "                            n_classes=n_classes,\n",
    "                            n_components=n_atoms,\n",
    "                            weight_initialization='uniform',\n",
    "                            n_distributions=len(Q),\n",
    "                            loss_fn=criterion,\n",
    "                            learning_rate_features=1e-1,\n",
    "                            learning_rate_labels=1e-1,\n",
    "                            learning_rate_weights=1e-1,\n",
    "                            reg_e=0.0,\n",
    "                            n_iter_barycenter=10,\n",
    "                            n_iter_sinkhorn=20,\n",
    "                            n_iter_emd=1000000,\n",
    "                            domain_names=None,\n",
    "                            grad_labels=True,\n",
    "                            optimizer_name='Adam',\n",
    "                            balanced_sampling=True,\n",
    "                            sampling_with_replacement=True,\n",
    "                            barycenter_tol=1e-9,\n",
    "                            barycenter_beta=None,\n",
    "                            tensor_dtype=torch.float32,\n",
    "                            track_atoms=False,\n",
    "                            schedule_lr=False)\n",
    "    dictionary.fit(Q,\n",
    "                n_iter_max=n_iter,\n",
    "                batches_per_it=n_samples // batch_size,\n",
    "                verbose=True)\n",
    "    weights = dictionary.A[-1, :].detach()\n",
    "    XP = [XPk.detach().clone() for XPk in dictionary.XP]\n",
    "    YP = [YPk.detach().clone().softmax(dim=-1) for YPk in dictionary.YP]\n",
    "    Xr, Yr = dictionary.reconstruct(weights=weights)\n",
    "\n",
    "    classifiers_e = {'lin': SVC(kernel='linear', probability=True), 'rbf': SVC(kernel='rbf', probability=True), 'RF': RandomForestClassifier()}\n",
    "    classifiers_r = {'lin': SVC(kernel='linear'), 'rbf': SVC(kernel='rbf',), 'RF': RandomForestClassifier()}\n",
    "\n",
    "    \n",
    "    for key in classifiers_e.keys():\n",
    "        # Without DA\n",
    "        clf_wda = classifiers_r[key]\n",
    "        clf_wda.fit(torch.cat(Xs, dim=0),\n",
    "                torch.cat(ys, dim=0))\n",
    "        yp = clf_wda.predict(Xt_test)\n",
    "        accuracy_wda = accuracy_score(yp, yt_test)\n",
    "        results[key]['wda'] += accuracy_wda\n",
    "\n",
    "        # DaDiL-E\n",
    "        clf_e = classifiers_e[key]\n",
    "        predictions = []\n",
    "        for XP_k, YP_k in zip(XP, YP):\n",
    "            # Get atom data\n",
    "            XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "            yp_k = YP_k.argmax(dim=1)\n",
    "            clf_e.fit(XP_k, yp_k)\n",
    "            P = clf_e.predict_proba(Xt_test)\n",
    "            predictions.append(P)\n",
    "        predictions = np.stack(predictions)\n",
    "        # Weights atomic model predictions\n",
    "        yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "        # Compute statistics\n",
    "        accuracy_e = accuracy_score(yt_test, yp)\n",
    "        results[key]['e'] += accuracy_e\n",
    "\n",
    "        # DaDiL-E with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            predictions = []\n",
    "            for XP_k, YP_k in zip(XP, YP):\n",
    "                # Get atom data\n",
    "                XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "                weights_k = torch.ones(XP_k.shape[0])/XP_k.shape[0]\n",
    "                weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "                C = torch.cdist(XP_k, Xt, p=2) ** 2\n",
    "                ot_plan = ot.emd(weights_k, weights_t, C, numItermax=1000000)\n",
    "                Yt = ot_plan.T @ YP_k\n",
    "                yt_k = Yt.argmax(dim=1)\n",
    "                clf_e.fit(Xt, yt_k)\n",
    "                P = clf_e.predict_proba(Xt_test)\n",
    "                predictions.append(P)\n",
    "            predictions = np.stack(predictions)\n",
    "            # Weights atomic model predictions\n",
    "            yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "            # Compute statistics\n",
    "            accuracy_e_ot = accuracy_score(yt_test, yp)\n",
    "            s += accuracy_e_ot\n",
    "        mean_accuracy_e_ot = s/10\n",
    "        results[key]['e_ot'] += mean_accuracy_e_ot\n",
    "\n",
    "        # DaDiL-R\n",
    "        clf_r = classifiers_r[key]\n",
    "        clf_r.fit(Xr, Yr.argmax(dim=1))\n",
    "        yp = clf_r.predict(Xt_test)\n",
    "        accuracy_r = accuracy_score(yp, yt_test)\n",
    "        results[key]['r'] += accuracy_r\n",
    "\n",
    "        # DaDiL-R with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            weights_r = torch.ones(Xr.shape[0])/Xr.shape[0]\n",
    "            weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "            C = torch.cdist(Xr, Xt, p=2) ** 2\n",
    "            ot_plan = ot.emd(weights_r, weights_t, C, numItermax=1000000)\n",
    "            Yt = ot_plan.T @ Yr\n",
    "            clf_r.fit(Xt, Yt.argmax(dim=1))\n",
    "            yp = clf_r.predict(Xt_test)\n",
    "            accuracy_r_ot = accuracy_score(yp, yt_test)\n",
    "            s += accuracy_r_ot\n",
    "        results[key]['r_ot'] += s/10\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 7250.020605468751\n",
      "It 2/100, Loss: 5412.389453125\n",
      "It 3/100, Loss: 3971.7236328125\n",
      "It 4/100, Loss: 2991.436328125\n",
      "It 5/100, Loss: 2328.8544921875\n",
      "It 6/100, Loss: 1843.2712158203126\n",
      "It 7/100, Loss: 1453.3168212890625\n",
      "It 8/100, Loss: 1166.7173095703124\n",
      "It 9/100, Loss: 945.2510131835938\n",
      "It 10/100, Loss: 764.6506469726563\n",
      "It 11/100, Loss: 632.8729858398438\n",
      "It 12/100, Loss: 540.3836059570312\n",
      "It 13/100, Loss: 466.92755126953125\n",
      "It 14/100, Loss: 401.4289916992187\n",
      "It 15/100, Loss: 359.0767761230469\n",
      "It 16/100, Loss: 323.2957702636719\n",
      "It 17/100, Loss: 291.80101318359374\n",
      "It 18/100, Loss: 274.8337951660156\n",
      "It 19/100, Loss: 265.99306640625\n",
      "It 20/100, Loss: 248.4516387939453\n",
      "It 21/100, Loss: 236.10530395507809\n",
      "It 22/100, Loss: 216.57101745605468\n",
      "It 23/100, Loss: 215.47197875976565\n",
      "It 24/100, Loss: 218.15243530273438\n",
      "It 25/100, Loss: 204.7766082763672\n",
      "It 26/100, Loss: 196.22800292968748\n",
      "It 27/100, Loss: 176.01340332031248\n",
      "It 28/100, Loss: 177.65384521484373\n",
      "It 29/100, Loss: 154.66368103027344\n",
      "It 30/100, Loss: 145.96284790039064\n",
      "It 31/100, Loss: 136.51913146972655\n",
      "It 32/100, Loss: 122.92336730957032\n",
      "It 33/100, Loss: 112.3841293334961\n",
      "It 34/100, Loss: 93.76186828613281\n",
      "It 35/100, Loss: 85.42312469482422\n",
      "It 36/100, Loss: 85.14025115966797\n",
      "It 37/100, Loss: 70.67264404296874\n",
      "It 38/100, Loss: 66.65522918701173\n",
      "It 39/100, Loss: 71.64189147949219\n",
      "It 40/100, Loss: 55.98991317749024\n",
      "It 41/100, Loss: 62.13569946289063\n",
      "It 42/100, Loss: 53.376240539550786\n",
      "It 43/100, Loss: 54.31305084228515\n",
      "It 44/100, Loss: 54.64499130249023\n",
      "It 45/100, Loss: 58.264253997802726\n",
      "It 46/100, Loss: 49.3300422668457\n",
      "It 47/100, Loss: 47.41453094482422\n",
      "It 48/100, Loss: 52.33143005371093\n",
      "It 49/100, Loss: 51.10355224609375\n",
      "It 50/100, Loss: 48.09014892578125\n",
      "It 51/100, Loss: 41.1709716796875\n",
      "It 52/100, Loss: 52.79581832885742\n",
      "It 53/100, Loss: 49.96847763061523\n",
      "It 54/100, Loss: 49.62427062988281\n",
      "It 55/100, Loss: 54.2779541015625\n",
      "It 56/100, Loss: 45.107575988769526\n",
      "It 57/100, Loss: 54.251435089111325\n",
      "It 58/100, Loss: 47.47493057250976\n",
      "It 59/100, Loss: 57.44671936035156\n",
      "It 60/100, Loss: 44.065493011474615\n",
      "It 61/100, Loss: 46.78319549560547\n",
      "It 62/100, Loss: 50.834466552734376\n",
      "It 63/100, Loss: 42.64332122802735\n",
      "It 64/100, Loss: 54.97294235229492\n",
      "It 65/100, Loss: 53.00731201171875\n",
      "It 66/100, Loss: 54.07461700439453\n",
      "It 67/100, Loss: 47.171531677246094\n",
      "It 68/100, Loss: 49.76932983398437\n",
      "It 69/100, Loss: 47.24900970458984\n",
      "It 70/100, Loss: 43.73310165405273\n",
      "It 71/100, Loss: 47.72489776611329\n",
      "It 72/100, Loss: 53.853503417968746\n",
      "It 73/100, Loss: 45.52610702514648\n",
      "It 74/100, Loss: 51.2351791381836\n",
      "It 75/100, Loss: 50.23790740966797\n",
      "It 76/100, Loss: 43.33857040405273\n",
      "It 77/100, Loss: 53.11155471801757\n",
      "It 78/100, Loss: 44.54601440429687\n",
      "It 79/100, Loss: 49.28320846557617\n",
      "It 80/100, Loss: 45.87078018188477\n",
      "It 81/100, Loss: 42.09465026855469\n",
      "It 82/100, Loss: 48.1850715637207\n",
      "It 83/100, Loss: 48.68442687988281\n",
      "It 84/100, Loss: 43.74929962158203\n",
      "It 85/100, Loss: 45.2032127380371\n",
      "It 86/100, Loss: 47.686710357666016\n",
      "It 87/100, Loss: 54.20997467041016\n",
      "It 88/100, Loss: 53.20070877075195\n",
      "It 89/100, Loss: 43.5092041015625\n",
      "It 90/100, Loss: 45.57951202392578\n",
      "It 91/100, Loss: 55.98236999511718\n",
      "It 92/100, Loss: 52.13733978271484\n",
      "It 93/100, Loss: 50.66790771484375\n",
      "It 94/100, Loss: 50.864803314208984\n",
      "It 95/100, Loss: 52.213166046142575\n",
      "It 96/100, Loss: 48.24402008056641\n",
      "It 97/100, Loss: 53.49468536376953\n",
      "It 98/100, Loss: 51.788049316406244\n",
      "It 99/100, Loss: 50.460486602783206\n",
      "It 100/100, Loss: 46.82860565185547\n"
     ]
    }
   ],
   "source": [
    "results = test_dadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'wda': 0.7316666666666667,\n",
       "  'e': 0.55125,\n",
       "  'e_ot': 0.9645000000000001,\n",
       "  'r': 0.9854166666666667,\n",
       "  'r_ot': 0.9695833333333332},\n",
       " 'rbf': {'wda': 0.72125,\n",
       "  'e': 0.8616666666666667,\n",
       "  'e_ot': 1.0,\n",
       "  'r': 1.0,\n",
       "  'r_ot': 1.0},\n",
       " 'RF': {'wda': 0.7129166666666666,\n",
       "  'e': 0.6879166666666666,\n",
       "  'e_ot': 0.9668749999999999,\n",
       "  'r': 0.9966666666666667,\n",
       "  'r_ot': 0.9345833333333331}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_odadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter):\n",
    "    results = {'lin':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'rbf':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'RF':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}}\n",
    "    \n",
    "    Q_sources = []\n",
    "    for Xs_k, ys_k in zip(Xs, ys):\n",
    "        Q_sources.append(\n",
    "            SupervisedDatasetMeasure(\n",
    "                features=Xs_k.numpy(),\n",
    "                labels=ys_k.numpy(),\n",
    "                stratify=True,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    criterion = JointWassersteinDistance()\n",
    "\n",
    "    dictionary_sources = LabeledDictionaryGMM(XP=None,\n",
    "                            YP=None,\n",
    "                            A=None,\n",
    "                            n_samples=n_samples,\n",
    "                            n_dim=n_features,\n",
    "                            n_classes=n_classes,\n",
    "                            n_components=n_atoms,\n",
    "                            weight_initialization='uniform',\n",
    "                            n_distributions=len(Q_sources),\n",
    "                            loss_fn=criterion,\n",
    "                            learning_rate_features=1e-1,\n",
    "                            learning_rate_labels=1e-1,\n",
    "                            learning_rate_weights=1e-1,\n",
    "                            reg_e=0.0,\n",
    "                            n_iter_barycenter=10,\n",
    "                            n_iter_sinkhorn=20,\n",
    "                            n_iter_emd=1000000,\n",
    "                            domain_names=None,\n",
    "                            grad_labels=True,\n",
    "                            optimizer_name='Adam',\n",
    "                            balanced_sampling=True,\n",
    "                            sampling_with_replacement=True,\n",
    "                            barycenter_tol=1e-9,\n",
    "                            barycenter_beta=None,\n",
    "                            tensor_dtype=torch.float32,\n",
    "                            track_atoms=False,\n",
    "                            schedule_lr=False)\n",
    "\n",
    "    dictionary_sources.fit(Q_sources,\n",
    "                n_iter_max=n_iter,\n",
    "                batches_per_it=n_samples // batch_size,\n",
    "                verbose=True)\n",
    "\n",
    "    XP_sources = dictionary_sources.XP\n",
    "    YP_sources = dictionary_sources.YP\n",
    "\n",
    "    dictionary_target = LabeledDictionaryGMM(XP=XP_sources,\n",
    "                                    YP=YP_sources,\n",
    "                                    A=None,\n",
    "                                    n_samples=n_samples,\n",
    "                                    n_dim=n_features,\n",
    "                                    n_classes=n_classes,\n",
    "                                    n_components=n_atoms,\n",
    "                                    weight_initialization='uniform',\n",
    "                                    n_distributions=1,\n",
    "                                    loss_fn=criterion,\n",
    "                                    learning_rate_features=0,\n",
    "                                    learning_rate_labels=0,\n",
    "                                    learning_rate_weights=1e-1,\n",
    "                                    reg_e=0.0,\n",
    "                                    n_iter_barycenter=10,\n",
    "                                    n_iter_sinkhorn=20,\n",
    "                                    n_iter_emd=1000000,\n",
    "                                    domain_names=None,\n",
    "                                    grad_labels=True,\n",
    "                                    optimizer_name='Adam',\n",
    "                                    balanced_sampling=True,\n",
    "                                    sampling_with_replacement=True,\n",
    "                                    barycenter_tol=1e-9,\n",
    "                                    barycenter_beta=None,\n",
    "                                    tensor_dtype=torch.float32,\n",
    "                                    track_atoms=False,\n",
    "                                    schedule_lr=False,\n",
    "                                    min_components=10,\n",
    "                                    max_step_components=10,\n",
    "                                    max_components=20)\n",
    "    \n",
    "    n_batch = 20\n",
    "    i = 0\n",
    "    while i < Xt.shape[0]-n_batch:\n",
    "        dictionary_target.fit_target_sample(Xt[i:i+n_batch, :],\n",
    "                                            batches_per_it=n_samples // batch_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                            verbose=True,\n",
    "                                            regularization=False,)\n",
    "        i += n_batch\n",
    "\n",
    "    weights = dictionary_target.A[-1, :].detach()\n",
    "    XP = [XPk.detach().clone() for XPk in dictionary_target.XP]\n",
    "    YP = [YPk.detach().clone().softmax(dim=-1) for YPk in dictionary_target.YP]\n",
    "\n",
    "    Xr, Yr = dictionary_target.reconstruct(weights=weights)\n",
    "\n",
    "    classifiers_e = {'lin': SVC(kernel='linear', probability=True), 'rbf': SVC(kernel='rbf', probability=True), 'RF': RandomForestClassifier()}\n",
    "    classifiers_r = {'lin': SVC(kernel='linear'), 'rbf': SVC(kernel='rbf',), 'RF': RandomForestClassifier()}\n",
    "\n",
    "    for key in classifiers_e.keys():\n",
    "        # Without DA\n",
    "        clf_wda = classifiers_r[key]\n",
    "        clf_wda.fit(torch.cat(Xs, dim=0),\n",
    "                torch.cat(ys, dim=0))\n",
    "        yp = clf_wda.predict(Xt_test)\n",
    "        accuracy_wda = accuracy_score(yp, yt_test)\n",
    "        results[key]['wda'] += accuracy_wda\n",
    "\n",
    "        #DaDiL-E\n",
    "        clf_e = classifiers_e[key]\n",
    "        predictions = []\n",
    "        for XP_k, YP_k in zip(XP, YP):\n",
    "            # Get atom data\n",
    "            XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "            yp_k = YP_k.argmax(dim=1)\n",
    "            clf_e.fit(XP_k, yp_k)\n",
    "            P = clf_e.predict_proba(Xt_test)\n",
    "            predictions.append(P)\n",
    "        predictions = np.stack(predictions)\n",
    "        # Weights atomic model predictions\n",
    "        yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "        # Compute statistics\n",
    "        accuracy_e = accuracy_score(yt_test, yp)\n",
    "        results[key]['e'] += accuracy_e\n",
    "\n",
    "        #DaDiL-E with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            predictions = []\n",
    "            for XP_k, YP_k in zip(XP, YP):\n",
    "                # Get atom data\n",
    "                XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "                weights_k = torch.ones(XP_k.shape[0])/XP_k.shape[0]\n",
    "                weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "                C = torch.cdist(XP_k, Xt, p=2) ** 2\n",
    "                ot_plan = ot.emd(weights_k, weights_t, C, numItermax=1000000)\n",
    "                Yt = ot_plan.T @ YP_k\n",
    "                yt_k = Yt.argmax(dim=1)\n",
    "                clf_e.fit(Xt, yt_k)\n",
    "                P = clf_e.predict_proba(Xt_test)\n",
    "                predictions.append(P)\n",
    "            predictions = np.stack(predictions)\n",
    "            # Weights atomic model predictions\n",
    "            yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "            # Compute statistics\n",
    "            accuracy_e_ot = accuracy_score(yt_test, yp)\n",
    "            s += accuracy_e_ot\n",
    "        results[key]['e_ot'] += s/10\n",
    "\n",
    "        #DaDiL-R\n",
    "        clf_r = classifiers_r[key]\n",
    "        clf_r.fit(Xr, Yr.argmax(dim=1))\n",
    "        yp = clf_r.predict(Xt_test)\n",
    "        accuracy_r = accuracy_score(yp, yt_test)\n",
    "        results[key]['r'] += accuracy_r\n",
    "\n",
    "        #DaDiL-R with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            weights_r = torch.ones(Xr.shape[0])/Xr.shape[0]\n",
    "            weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "            C = torch.cdist(Xr, Xt, p=2) ** 2\n",
    "            ot_plan = ot.emd(weights_r, weights_t, C, numItermax=1000000)\n",
    "            Yt = ot_plan.T @ Yr\n",
    "            clf_r.fit(Xt, Yt.argmax(dim=1))\n",
    "            yp = clf_r.predict(Xt_test)\n",
    "            accuracy_r_ot = accuracy_score(yp, yt_test)\n",
    "            s += accuracy_r_ot\n",
    "        results[key]['r_ot'] += s/10\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 6155.6748046875\n",
      "It 2/100, Loss: 4598.67529296875\n",
      "It 3/100, Loss: 3395.6925781249997\n",
      "It 4/100, Loss: 2567.266064453125\n",
      "It 5/100, Loss: 2030.51181640625\n",
      "It 6/100, Loss: 1634.2913574218749\n",
      "It 7/100, Loss: 1315.9779296875\n",
      "It 8/100, Loss: 1076.7628173828125\n",
      "It 9/100, Loss: 885.8607177734375\n",
      "It 10/100, Loss: 720.4839965820313\n",
      "It 11/100, Loss: 579.9966430664062\n",
      "It 12/100, Loss: 482.60099487304683\n",
      "It 13/100, Loss: 398.63926391601564\n",
      "It 14/100, Loss: 329.35023193359376\n",
      "It 15/100, Loss: 274.9776092529297\n",
      "It 16/100, Loss: 231.20325622558593\n",
      "It 17/100, Loss: 196.91969299316406\n",
      "It 18/100, Loss: 168.05216064453126\n",
      "It 19/100, Loss: 144.07355346679688\n",
      "It 20/100, Loss: 124.32376251220703\n",
      "It 21/100, Loss: 108.23246765136719\n",
      "It 22/100, Loss: 93.77239532470702\n",
      "It 23/100, Loss: 82.05999145507812\n",
      "It 24/100, Loss: 73.50467834472657\n",
      "It 25/100, Loss: 65.47496948242187\n",
      "It 26/100, Loss: 59.25986175537109\n",
      "It 27/100, Loss: 54.32483749389648\n",
      "It 28/100, Loss: 49.90577697753907\n",
      "It 29/100, Loss: 45.15039367675781\n",
      "It 30/100, Loss: 42.3237907409668\n",
      "It 31/100, Loss: 39.288753509521484\n",
      "It 32/100, Loss: 36.89488525390625\n",
      "It 33/100, Loss: 34.84229354858398\n",
      "It 34/100, Loss: 32.94188919067383\n",
      "It 35/100, Loss: 30.841231536865237\n",
      "It 36/100, Loss: 29.730683135986332\n",
      "It 37/100, Loss: 28.505483627319336\n",
      "It 38/100, Loss: 27.08347816467285\n",
      "It 39/100, Loss: 26.80502853393555\n",
      "It 40/100, Loss: 24.86370506286621\n",
      "It 41/100, Loss: 24.392975234985354\n",
      "It 42/100, Loss: 24.168772125244143\n",
      "It 43/100, Loss: 22.96298179626465\n",
      "It 44/100, Loss: 22.75774574279785\n",
      "It 45/100, Loss: 22.187494659423827\n",
      "It 46/100, Loss: 21.55675010681152\n",
      "It 47/100, Loss: 20.94893112182617\n",
      "It 48/100, Loss: 19.943537521362305\n",
      "It 49/100, Loss: 20.018668746948244\n",
      "It 50/100, Loss: 19.784804153442384\n",
      "It 51/100, Loss: 19.25259819030762\n",
      "It 52/100, Loss: 18.98962745666504\n",
      "It 53/100, Loss: 18.995702362060545\n",
      "It 54/100, Loss: 17.826275634765626\n",
      "It 55/100, Loss: 17.759577560424802\n",
      "It 56/100, Loss: 17.491099548339847\n",
      "It 57/100, Loss: 17.34981918334961\n",
      "It 58/100, Loss: 17.042478561401367\n",
      "It 59/100, Loss: 16.25548915863037\n",
      "It 60/100, Loss: 15.68280029296875\n",
      "It 61/100, Loss: 15.64745578765869\n",
      "It 62/100, Loss: 14.94286346435547\n",
      "It 63/100, Loss: 14.615266036987304\n",
      "It 64/100, Loss: 14.112377929687499\n",
      "It 65/100, Loss: 13.867073059082031\n",
      "It 66/100, Loss: 13.869537925720214\n",
      "It 67/100, Loss: 12.601911926269532\n",
      "It 68/100, Loss: 12.849240112304688\n",
      "It 69/100, Loss: 12.189286422729491\n",
      "It 70/100, Loss: 11.819058227539063\n",
      "It 71/100, Loss: 11.934999656677247\n",
      "It 72/100, Loss: 11.563245391845705\n",
      "It 73/100, Loss: 11.636749076843262\n",
      "It 74/100, Loss: 11.259424591064453\n",
      "It 75/100, Loss: 10.8255859375\n",
      "It 76/100, Loss: 10.680182647705077\n",
      "It 77/100, Loss: 10.885543632507325\n",
      "It 78/100, Loss: 10.919767951965333\n",
      "It 79/100, Loss: 10.510935401916504\n",
      "It 80/100, Loss: 10.469466590881348\n",
      "It 81/100, Loss: 10.20966625213623\n",
      "It 82/100, Loss: 10.204222297668458\n",
      "It 83/100, Loss: 10.062644958496094\n",
      "It 84/100, Loss: 10.167588806152343\n",
      "It 85/100, Loss: 10.160124969482421\n",
      "It 86/100, Loss: 10.17272415161133\n",
      "It 87/100, Loss: 9.88000431060791\n",
      "It 88/100, Loss: 10.094350433349609\n",
      "It 89/100, Loss: 9.853683853149414\n",
      "It 90/100, Loss: 9.926400947570801\n",
      "It 91/100, Loss: 10.12913475036621\n",
      "It 92/100, Loss: 9.921275711059568\n",
      "It 93/100, Loss: 9.884283447265625\n",
      "It 94/100, Loss: 9.861235618591309\n",
      "It 95/100, Loss: 9.78695011138916\n",
      "It 96/100, Loss: 9.762707710266113\n",
      "It 97/100, Loss: 9.717987251281738\n",
      "It 98/100, Loss: 9.518183517456054\n",
      "It 99/100, Loss: 9.459481811523437\n",
      "It 100/100, Loss: 9.641897773742675\n",
      "Loss: 299.0173645019531\n",
      "Loss: 266.88140869140625\n",
      "Loss: 265.43690795898436\n",
      "Loss: 270.4948547363281\n",
      "Loss: 262.02625427246096\n",
      "Loss: 274.3633544921875\n",
      "Loss: 273.76551818847656\n",
      "Loss: 270.6269470214844\n",
      "Loss: 265.9637084960938\n",
      "Loss: 285.0182678222656\n",
      "Loss: 284.5589294433594\n",
      "Loss: 280.31215209960936\n",
      "Loss: 279.516650390625\n",
      "Loss: 284.60422973632814\n",
      "Loss: 292.35007934570314\n",
      "Loss: 291.86578979492185\n",
      "Loss: 301.4097412109375\n",
      "Loss: 294.5171264648437\n",
      "Loss: 295.78278198242185\n",
      "Loss: 299.2367309570312\n",
      "Loss: 302.1088439941406\n",
      "Loss: 312.39856567382816\n",
      "Loss: 313.8991333007813\n",
      "Loss: 307.31313476562497\n",
      "Loss: 312.99553833007815\n",
      "Loss: 310.52564697265626\n",
      "Loss: 312.61708374023436\n",
      "Loss: 312.8570068359375\n",
      "Loss: 327.194140625\n",
      "Loss: 324.30656127929683\n",
      "Loss: 329.27960205078125\n",
      "Loss: 328.411865234375\n",
      "Loss: 332.47734985351565\n",
      "Loss: 325.72138061523435\n",
      "Loss: 337.47576904296875\n",
      "Loss: 335.7043151855469\n",
      "Loss: 338.374951171875\n",
      "Loss: 342.3658020019531\n",
      "Loss: 336.81980590820314\n",
      "Loss: 341.192236328125\n",
      "Loss: 339.2429443359375\n",
      "Loss: 343.9157409667969\n",
      "Loss: 348.28301391601565\n",
      "Loss: 340.5000122070313\n",
      "Loss: 344.3393737792968\n",
      "Loss: 353.22155151367184\n",
      "Loss: 343.7\n",
      "Loss: 346.89165039062505\n",
      "Loss: 353.97963867187497\n",
      "Loss: 360.8475708007813\n",
      "Loss: 354.47747802734375\n",
      "Loss: 354.13263549804685\n",
      "Loss: 359.4786743164062\n",
      "Loss: 367.5855285644531\n",
      "Loss: 367.36981811523435\n",
      "Loss: 356.5289672851562\n",
      "Loss: 356.49541015625\n",
      "Loss: 361.90269775390624\n",
      "Loss: 365.34578247070317\n",
      "Loss: 356.75419921875\n",
      "Loss: 357.97557373046874\n",
      "Loss: 357.99464111328126\n",
      "Loss: 359.2126098632813\n",
      "Loss: 363.7762268066406\n",
      "Loss: 355.7360595703125\n",
      "Loss: 361.9207946777344\n",
      "Loss: 360.0172180175781\n",
      "Loss: 362.9441284179688\n",
      "Loss: 363.2665161132812\n",
      "Loss: 362.4548706054687\n",
      "Loss: 356.82982177734374\n",
      "Loss: 358.27860717773433\n",
      "Loss: 362.54536743164067\n",
      "Loss: 365.1097473144531\n",
      "Loss: 359.09306640625005\n",
      "Loss: 356.4593627929687\n",
      "Loss: 370.74876708984374\n",
      "Loss: 366.79006958007807\n",
      "Loss: 362.43562011718745\n",
      "Loss: 361.57542114257814\n",
      "Loss: 368.69420166015624\n",
      "Loss: 374.4868225097656\n",
      "Loss: 367.7042297363282\n",
      "Loss: 374.3010559082031\n",
      "Loss: 368.2455688476563\n",
      "Loss: 372.70974121093747\n",
      "Loss: 367.5306945800781\n",
      "Loss: 366.60716552734374\n",
      "Loss: 371.3668090820313\n",
      "Loss: 359.4022888183594\n",
      "Loss: 372.0216247558594\n",
      "Loss: 368.46192626953126\n",
      "Loss: 368.81769409179685\n",
      "Loss: 365.76715698242185\n",
      "Loss: 368.5645446777343\n",
      "Loss: 365.28901977539067\n",
      "Loss: 359.0945678710938\n",
      "Loss: 365.4940612792969\n",
      "Loss: 363.5273132324219\n",
      "Loss: 368.43121337890625\n",
      "Loss: 366.05010375976565\n",
      "Loss: 371.244287109375\n",
      "Loss: 362.80065917968744\n",
      "Loss: 362.75422363281245\n",
      "Loss: 365.43452758789067\n",
      "Loss: 364.77313842773435\n",
      "Loss: 366.18626708984374\n",
      "Loss: 357.6463500976562\n",
      "Loss: 363.168701171875\n",
      "Loss: 365.970751953125\n",
      "Loss: 366.0167785644531\n",
      "Loss: 379.0947998046875\n",
      "Loss: 371.52107543945317\n",
      "Loss: 370.4488525390625\n",
      "Loss: 374.20549926757815\n",
      "Loss: 371.660009765625\n",
      "Loss: 374.4740295410157\n",
      "Loss: 375.59407348632817\n",
      "Loss: 375.2950866699219\n",
      "Loss: 370.2053283691406\n",
      "Loss: 368.1824584960938\n",
      "Loss: 369.2340026855469\n",
      "Loss: 371.00090332031255\n",
      "Loss: 367.701171875\n",
      "Loss: 366.11024169921876\n",
      "Loss: 364.00232543945316\n",
      "Loss: 369.22096557617186\n",
      "Loss: 367.91156005859375\n",
      "Loss: 365.07265625\n",
      "Loss: 369.84528198242185\n",
      "Loss: 368.7275756835937\n",
      "Loss: 366.6215270996094\n",
      "Loss: 372.7904602050781\n",
      "Loss: 357.6230163574219\n",
      "Loss: 367.5098815917969\n",
      "Loss: 367.73157348632816\n",
      "Loss: 368.17826538085933\n",
      "Loss: 361.6677612304688\n",
      "Loss: 369.6005004882812\n",
      "Loss: 373.14655761718757\n",
      "Loss: 370.8507873535156\n",
      "Loss: 365.63115844726565\n",
      "Loss: 369.94366455078125\n",
      "Loss: 367.4773315429687\n",
      "Loss: 364.30352783203125\n",
      "Loss: 369.27092895507815\n",
      "Loss: 373.88580322265625\n",
      "Loss: 379.9989379882812\n",
      "Loss: 373.2936279296875\n",
      "Loss: 367.4451293945312\n",
      "Loss: 376.9747436523438\n",
      "Loss: 377.65003662109376\n",
      "Loss: 375.3809509277344\n",
      "Loss: 375.36083984375\n",
      "Loss: 381.12009887695314\n",
      "Loss: 373.0296875\n",
      "Loss: 373.0136535644531\n",
      "Loss: 372.7825500488281\n",
      "Loss: 375.9924072265625\n",
      "Loss: 370.15080566406255\n",
      "Loss: 372.6032775878906\n",
      "Loss: 373.0986755371094\n",
      "Loss: 373.4372436523438\n",
      "Loss: 375.96550903320315\n",
      "Loss: 367.0871826171875\n",
      "Loss: 377.4370849609375\n",
      "Loss: 373.7064575195312\n",
      "Loss: 369.2115905761719\n",
      "Loss: 376.4852783203125\n",
      "Loss: 372.5070129394531\n",
      "Loss: 374.71477050781255\n",
      "Loss: 377.21596679687497\n",
      "Loss: 369.0655029296875\n",
      "Loss: 373.6066528320313\n",
      "Loss: 374.4081787109375\n",
      "Loss: 365.64255371093753\n",
      "Loss: 368.35601196289065\n",
      "Loss: 371.66022338867185\n",
      "Loss: 375.2663879394531\n",
      "Loss: 374.2520263671875\n",
      "Loss: 371.8955078125\n",
      "Loss: 373.2756713867187\n",
      "Loss: 375.0620727539062\n",
      "Loss: 374.1298889160156\n",
      "Loss: 370.0306457519531\n",
      "Loss: 370.00376586914064\n",
      "Loss: 369.0269165039063\n",
      "Loss: 371.96196289062505\n",
      "Loss: 375.2026550292969\n",
      "Loss: 373.0794921875\n",
      "Loss: 377.76600952148436\n",
      "Loss: 372.98061523437497\n",
      "Loss: 377.0897705078125\n",
      "Loss: 377.9101745605469\n",
      "Loss: 378.3073974609375\n",
      "Loss: 372.9182495117187\n",
      "Loss: 372.73845825195315\n",
      "Loss: 376.0650634765625\n",
      "Loss: 369.14044799804685\n",
      "Loss: 376.8783142089844\n",
      "Loss: 368.61276245117193\n",
      "Loss: 369.832763671875\n",
      "Loss: 372.79880981445314\n",
      "Loss: 369.35462646484376\n",
      "Loss: 370.6358276367188\n",
      "Loss: 367.2403259277343\n",
      "Loss: 364.85192260742184\n",
      "Loss: 374.4003479003906\n",
      "Loss: 375.38783569335936\n",
      "Loss: 374.3956665039062\n",
      "Loss: 369.4809936523437\n",
      "Loss: 367.0746215820312\n",
      "Loss: 369.3228698730469\n",
      "Loss: 364.5735473632813\n",
      "Loss: 371.57603759765624\n",
      "Loss: 367.60357055664065\n",
      "Loss: 372.9887145996094\n",
      "Loss: 380.9555541992188\n",
      "Loss: 364.71638793945317\n",
      "Loss: 371.5614440917969\n",
      "Loss: 369.42911987304694\n",
      "Loss: 370.9765197753906\n",
      "Loss: 372.96063232421875\n",
      "Loss: 369.9705505371094\n",
      "Loss: 364.76806640625\n",
      "Loss: 368.65838012695315\n",
      "Loss: 375.256396484375\n",
      "Loss: 375.4875061035156\n",
      "Loss: 380.44786376953124\n",
      "Loss: 378.6401550292969\n",
      "Loss: 376.2956481933594\n",
      "Loss: 370.2510681152344\n",
      "Loss: 376.3667785644531\n",
      "Loss: 367.4439270019531\n",
      "Loss: 371.0542846679688\n",
      "Loss: 372.85930786132815\n",
      "Loss: 372.64256591796874\n",
      "Loss: 373.1682922363281\n",
      "Loss: 378.5403747558594\n",
      "Loss: 363.85053100585935\n",
      "Loss: 367.3010314941406\n",
      "Loss: 370.17893676757814\n",
      "Loss: 371.8412109375\n",
      "Loss: 373.75721435546876\n",
      "Loss: 374.99470825195317\n",
      "Loss: 369.079052734375\n",
      "Loss: 380.89389648437503\n",
      "Loss: 373.8856201171875\n",
      "Loss: 377.8796142578125\n",
      "Loss: 371.24363403320314\n",
      "Loss: 379.1131286621094\n",
      "Loss: 378.7942810058594\n",
      "Loss: 376.6526062011719\n",
      "Loss: 376.3533142089844\n",
      "Loss: 374.8833862304687\n",
      "Loss: 376.2273864746094\n",
      "Loss: 367.2505310058594\n",
      "Loss: 372.0642456054687\n",
      "Loss: 376.6109069824219\n",
      "Loss: 371.740625\n",
      "Loss: 371.4957946777344\n",
      "Loss: 370.67803344726565\n",
      "Loss: 375.1450256347656\n",
      "Loss: 371.90797119140626\n",
      "Loss: 368.7245971679687\n",
      "Loss: 371.1410705566406\n",
      "Loss: 372.5430053710937\n",
      "Loss: 372.5386535644531\n",
      "Loss: 373.9114135742187\n",
      "Loss: 369.83558349609376\n",
      "Loss: 375.580322265625\n",
      "Loss: 373.3565673828125\n",
      "Loss: 367.0654663085937\n",
      "Loss: 369.9529174804688\n",
      "Loss: 373.164794921875\n",
      "Loss: 369.8204650878906\n",
      "Loss: 371.55804443359375\n",
      "Loss: 364.56537475585935\n",
      "Loss: 368.30849609375\n"
     ]
    }
   ],
   "source": [
    "results_odadil = test_odadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'wda': 0.7316666666666667,\n",
       "  'e': 0.7529166666666667,\n",
       "  'e_ot': 0.9854166666666668,\n",
       "  'r': 0.7520833333333333,\n",
       "  'r_ot': 0.985},\n",
       " 'rbf': {'wda': 0.72125,\n",
       "  'e': 0.75625,\n",
       "  'e_ot': 1.0,\n",
       "  'r': 0.6995833333333333,\n",
       "  'r_ot': 1.0},\n",
       " 'RF': {'wda': 0.6883333333333334,\n",
       "  'e': 0.6829166666666666,\n",
       "  'e_ot': 0.990375,\n",
       "  'r': 0.5975,\n",
       "  'r_ot': 0.9896666666666667}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_odadil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forgetting_odadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter):\n",
    "    before_online_results = {'lin':{'r':[], 'r_ot':[]}, \n",
    "               'rbf':{'r':[], 'r_ot':[]}, \n",
    "               'RF':{'r':[], 'r_ot':[]}}\n",
    "    after_online_results = {'lin':{'r':[], 'r_ot':[]}, \n",
    "               'rbf':{'r':[], 'r_ot':[]}, \n",
    "               'RF':{'r':[], 'r_ot':[]}}\n",
    "    classifiers_r = {'lin': SVC(kernel='linear'), 'rbf': SVC(kernel='rbf',), 'RF': RandomForestClassifier()}\n",
    "\n",
    "    Q_sources = []\n",
    "    for Xs_k, ys_k in zip(Xs, ys):\n",
    "        Q_sources.append(\n",
    "            SupervisedDatasetMeasure(\n",
    "                features=Xs_k.numpy(),\n",
    "                labels=ys_k.numpy(),\n",
    "                stratify=True,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    criterion = JointWassersteinDistance()\n",
    "\n",
    "    dictionary_sources = LabeledDictionaryGMM(XP=None,\n",
    "                            YP=None,\n",
    "                            A=None,\n",
    "                            n_samples=n_samples,\n",
    "                            n_dim=n_features,\n",
    "                            n_classes=n_classes,\n",
    "                            n_components=n_atoms,\n",
    "                            weight_initialization='uniform',\n",
    "                            n_distributions=len(Q_sources),\n",
    "                            loss_fn=criterion,\n",
    "                            learning_rate_features=1e-1,\n",
    "                            learning_rate_labels=1e-1,\n",
    "                            learning_rate_weights=1e-1,\n",
    "                            reg_e=0.0,\n",
    "                            n_iter_barycenter=10,\n",
    "                            n_iter_sinkhorn=20,\n",
    "                            n_iter_emd=1000000,\n",
    "                            domain_names=None,\n",
    "                            grad_labels=True,\n",
    "                            optimizer_name='Adam',\n",
    "                            balanced_sampling=True,\n",
    "                            sampling_with_replacement=True,\n",
    "                            barycenter_tol=1e-9,\n",
    "                            barycenter_beta=None,\n",
    "                            tensor_dtype=torch.float32,\n",
    "                            track_atoms=False,\n",
    "                            schedule_lr=False)\n",
    "\n",
    "    dictionary_sources.fit(Q_sources,\n",
    "                n_iter_max=n_iter,\n",
    "                batches_per_it=n_samples // batch_size,\n",
    "                verbose=True)\n",
    "\n",
    "    XP_sources = dictionary_sources.XP\n",
    "    YP_sources = dictionary_sources.YP\n",
    "\n",
    "    weights_list = dictionary_sources.A.detach()\n",
    "\n",
    "    # Test classif sources avant OGMM\n",
    "    for i in range(len(weights_list)):\n",
    "        Xr, Yr = dictionary_sources.reconstruct(weights=weights_list[i])\n",
    "\n",
    "        for key in classifiers_r.keys():\n",
    "            #DaDiL-R\n",
    "            clf_r = classifiers_r[key]\n",
    "            clf_r.fit(Xr, Yr.argmax(dim=1))\n",
    "            yp = clf_r.predict(Xt_test)\n",
    "            accuracy_r = accuracy_score(yp, yt_test)\n",
    "            before_online_results[key]['r'].append(accuracy_r)\n",
    "\n",
    "            #DaDiL-R with last optimal transport\n",
    "            s = 0\n",
    "            for _ in range(10):\n",
    "                weights_r = torch.ones(Xr.shape[0])/Xr.shape[0]\n",
    "                weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "                C = torch.cdist(Xr, Xt, p=2) ** 2\n",
    "                ot_plan = ot.emd(weights_r, weights_t, C, numItermax=1000000)\n",
    "                Yt = ot_plan.T @ Yr\n",
    "                clf_r.fit(Xt, Yt.argmax(dim=1))\n",
    "                yp = clf_r.predict(Xt_test)\n",
    "                accuracy_r_ot = accuracy_score(yp, yt_test)\n",
    "                s += accuracy_r_ot\n",
    "            before_online_results[key]['r_ot'].append(s/10)\n",
    "\n",
    "    # Online Learning\n",
    "    dictionary_target = LabeledDictionaryGMM(XP=XP_sources,\n",
    "                                    YP=YP_sources,\n",
    "                                    A=None,\n",
    "                                    n_samples=n_samples,\n",
    "                                    n_dim=n_features,\n",
    "                                    n_classes=n_classes,\n",
    "                                    n_components=n_atoms,\n",
    "                                    weight_initialization='uniform',\n",
    "                                    n_distributions=1,\n",
    "                                    loss_fn=criterion,\n",
    "                                    learning_rate_features=0,\n",
    "                                    learning_rate_labels=0,\n",
    "                                    learning_rate_weights=1e-1,\n",
    "                                    reg_e=0.0,\n",
    "                                    n_iter_barycenter=10,\n",
    "                                    n_iter_sinkhorn=20,\n",
    "                                    n_iter_emd=1000000,\n",
    "                                    domain_names=None,\n",
    "                                    grad_labels=True,\n",
    "                                    optimizer_name='Adam',\n",
    "                                    balanced_sampling=True,\n",
    "                                    sampling_with_replacement=True,\n",
    "                                    barycenter_tol=1e-9,\n",
    "                                    barycenter_beta=None,\n",
    "                                    tensor_dtype=torch.float32,\n",
    "                                    track_atoms=False,\n",
    "                                    schedule_lr=False,\n",
    "                                    min_components=10,\n",
    "                                    max_step_components=10,\n",
    "                                    max_components=20)\n",
    "    \n",
    "    n_batch = 20\n",
    "    c = 0\n",
    "    while c < Xt.shape[0]-n_batch:\n",
    "        dictionary_target.fit_target_sample(Xt[c:c+n_batch, :],\n",
    "                                            batches_per_it=n_samples // batch_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                            verbose=True,\n",
    "                                            regularization=False,)\n",
    "        c += n_batch\n",
    "\n",
    "\n",
    "    # Test classif sources aprÃ¨s online learning\n",
    "    for i in range(len(weights_list)):\n",
    "        Xr, Yr = dictionary_target.reconstruct(weights=weights_list[i])\n",
    "\n",
    "        for key in classifiers_r.keys():\n",
    "            #DaDiL-R\n",
    "            clf_r = classifiers_r[key]\n",
    "            clf_r.fit(Xr, Yr.argmax(dim=1))\n",
    "            yp = clf_r.predict(Xt_test)\n",
    "            accuracy_r = accuracy_score(yp, yt_test)\n",
    "            after_online_results[key]['r'].append(accuracy_r)\n",
    "\n",
    "            #DaDiL-R with last optimal transport\n",
    "            s = 0\n",
    "            for _ in range(10):\n",
    "                weights_r = torch.ones(Xr.shape[0])/Xr.shape[0]\n",
    "                weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "                C = torch.cdist(Xr, Xt, p=2) ** 2\n",
    "                ot_plan = ot.emd(weights_r, weights_t, C, numItermax=1000000)\n",
    "                Yt = ot_plan.T @ Yr\n",
    "                clf_r.fit(Xt, Yt.argmax(dim=1))\n",
    "                yp = clf_r.predict(Xt_test)\n",
    "                accuracy_r_ot = accuracy_score(yp, yt_test)\n",
    "                s += accuracy_r_ot\n",
    "            after_online_results[key]['r_ot'].append(s/10)\n",
    "    \n",
    "    return before_online_results, after_online_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 6184.32822265625\n",
      "It 2/100, Loss: 4652.443798828125\n",
      "It 3/100, Loss: 3393.0194335937495\n",
      "It 4/100, Loss: 2564.2015625\n",
      "It 5/100, Loss: 2066.425610351562\n",
      "It 6/100, Loss: 1667.5993896484376\n",
      "It 7/100, Loss: 1336.59296875\n",
      "It 8/100, Loss: 1108.1639404296875\n",
      "It 9/100, Loss: 877.7685546875\n",
      "It 10/100, Loss: 731.2276367187501\n",
      "It 11/100, Loss: 601.156787109375\n",
      "It 12/100, Loss: 491.2104309082031\n",
      "It 13/100, Loss: 402.89794921875\n",
      "It 14/100, Loss: 338.68431396484374\n",
      "It 15/100, Loss: 283.22911376953124\n",
      "It 16/100, Loss: 236.00957946777345\n",
      "It 17/100, Loss: 198.70504760742188\n",
      "It 18/100, Loss: 171.68735961914064\n",
      "It 19/100, Loss: 145.5631591796875\n",
      "It 20/100, Loss: 125.6956069946289\n",
      "It 21/100, Loss: 108.38717498779296\n",
      "It 22/100, Loss: 95.46341094970703\n",
      "It 23/100, Loss: 84.44511718749999\n",
      "It 24/100, Loss: 73.37073211669922\n",
      "It 25/100, Loss: 66.61158294677735\n",
      "It 26/100, Loss: 60.346449279785155\n",
      "It 27/100, Loss: 54.38810729980469\n",
      "It 28/100, Loss: 49.41547775268555\n",
      "It 29/100, Loss: 45.52946395874024\n",
      "It 30/100, Loss: 42.6075439453125\n",
      "It 31/100, Loss: 39.52621002197266\n",
      "It 32/100, Loss: 37.53145828247071\n",
      "It 33/100, Loss: 35.23646545410156\n",
      "It 34/100, Loss: 33.349440765380855\n",
      "It 35/100, Loss: 31.093265914916994\n",
      "It 36/100, Loss: 29.73342628479004\n",
      "It 37/100, Loss: 28.500481033325194\n",
      "It 38/100, Loss: 26.861692428588867\n",
      "It 39/100, Loss: 26.36269340515137\n",
      "It 40/100, Loss: 25.137989044189453\n",
      "It 41/100, Loss: 24.393816757202146\n",
      "It 42/100, Loss: 23.986034393310547\n",
      "It 43/100, Loss: 23.250101470947268\n",
      "It 44/100, Loss: 22.58517837524414\n",
      "It 45/100, Loss: 21.90690383911133\n",
      "It 46/100, Loss: 22.03286361694336\n",
      "It 47/100, Loss: 21.468798065185545\n",
      "It 48/100, Loss: 20.44243278503418\n",
      "It 49/100, Loss: 20.074747467041014\n",
      "It 50/100, Loss: 20.21269302368164\n",
      "It 51/100, Loss: 19.40632400512695\n",
      "It 52/100, Loss: 18.865655517578126\n",
      "It 53/100, Loss: 18.789669036865234\n",
      "It 54/100, Loss: 18.40639305114746\n",
      "It 55/100, Loss: 17.681084060668944\n",
      "It 56/100, Loss: 17.534417724609373\n",
      "It 57/100, Loss: 16.610033416748045\n",
      "It 58/100, Loss: 16.62150001525879\n",
      "It 59/100, Loss: 17.693995857238768\n",
      "It 60/100, Loss: 17.735040855407714\n",
      "It 61/100, Loss: 15.039641571044921\n",
      "It 62/100, Loss: 15.056074905395507\n",
      "It 63/100, Loss: 14.431840896606445\n",
      "It 64/100, Loss: 14.416720199584962\n",
      "It 65/100, Loss: 13.456555557250978\n",
      "It 66/100, Loss: 12.973216819763184\n",
      "It 67/100, Loss: 12.777938461303712\n",
      "It 68/100, Loss: 12.122994422912598\n",
      "It 69/100, Loss: 12.180326461791992\n",
      "It 70/100, Loss: 11.796463775634765\n",
      "It 71/100, Loss: 11.261843490600585\n",
      "It 72/100, Loss: 11.227416801452637\n",
      "It 73/100, Loss: 10.924189949035645\n",
      "It 74/100, Loss: 10.76777629852295\n",
      "It 75/100, Loss: 10.598272514343261\n",
      "It 76/100, Loss: 10.791276741027833\n",
      "It 77/100, Loss: 10.594203186035156\n",
      "It 78/100, Loss: 10.602346992492677\n",
      "It 79/100, Loss: 10.253034782409667\n",
      "It 80/100, Loss: 10.2748592376709\n",
      "It 81/100, Loss: 10.640059089660642\n",
      "It 82/100, Loss: 10.117814636230468\n",
      "It 83/100, Loss: 10.280698013305663\n",
      "It 84/100, Loss: 10.11761417388916\n",
      "It 85/100, Loss: 10.103105545043945\n",
      "It 86/100, Loss: 10.044147872924805\n",
      "It 87/100, Loss: 10.12775115966797\n",
      "It 88/100, Loss: 9.861527252197266\n",
      "It 89/100, Loss: 9.694173431396484\n",
      "It 90/100, Loss: 9.813709449768066\n",
      "It 91/100, Loss: 9.66667881011963\n",
      "It 92/100, Loss: 9.811929893493653\n",
      "It 93/100, Loss: 9.818051719665528\n",
      "It 94/100, Loss: 9.912926483154298\n",
      "It 95/100, Loss: 9.994480323791503\n",
      "It 96/100, Loss: 9.767912483215332\n",
      "It 97/100, Loss: 9.408371353149414\n",
      "It 98/100, Loss: 9.578058624267577\n",
      "It 99/100, Loss: 9.697763442993164\n",
      "It 100/100, Loss: 9.432602691650391\n",
      "Loss: 276.33265380859376\n",
      "Loss: 276.0350769042969\n",
      "Loss: 263.5084991455078\n",
      "Loss: 268.9458557128906\n",
      "Loss: 278.40107421875\n",
      "Loss: 269.6978332519531\n",
      "Loss: 277.56181640625\n",
      "Loss: 270.15453491210934\n",
      "Loss: 274.17557983398433\n",
      "Loss: 267.1866882324219\n",
      "Loss: 272.79445190429686\n",
      "Loss: 281.48609619140626\n",
      "Loss: 279.8794738769531\n",
      "Loss: 289.686962890625\n",
      "Loss: 286.5314819335938\n",
      "Loss: 293.49596557617184\n",
      "Loss: 294.56723632812503\n",
      "Loss: 298.7164001464844\n",
      "Loss: 295.643701171875\n",
      "Loss: 299.25245361328126\n",
      "Loss: 304.4778869628906\n",
      "Loss: 311.03880615234374\n",
      "Loss: 313.0463134765625\n",
      "Loss: 303.0845458984375\n",
      "Loss: 317.2828491210937\n",
      "Loss: 317.8937561035156\n",
      "Loss: 320.7872619628906\n",
      "Loss: 319.1253234863281\n",
      "Loss: 321.72870483398435\n",
      "Loss: 321.75415649414066\n",
      "Loss: 322.9761657714844\n",
      "Loss: 326.2648986816406\n",
      "Loss: 334.3746765136719\n",
      "Loss: 332.52816772460943\n",
      "Loss: 331.2035278320312\n",
      "Loss: 331.5785339355469\n",
      "Loss: 340.5694519042969\n",
      "Loss: 339.86450805664066\n",
      "Loss: 337.23291015625\n",
      "Loss: 342.2174926757813\n",
      "Loss: 339.48748779296875\n",
      "Loss: 345.4107299804688\n",
      "Loss: 345.9547424316406\n",
      "Loss: 348.0892883300781\n",
      "Loss: 354.4385803222656\n",
      "Loss: 352.7915954589844\n",
      "Loss: 350.9822204589844\n",
      "Loss: 350.74107666015624\n",
      "Loss: 352.855908203125\n",
      "Loss: 350.97929687500005\n",
      "Loss: 353.3250122070312\n",
      "Loss: 361.02955932617186\n",
      "Loss: 364.57661132812507\n",
      "Loss: 353.9004028320312\n",
      "Loss: 361.57916870117185\n",
      "Loss: 354.67887573242183\n",
      "Loss: 356.4855529785156\n",
      "Loss: 354.25164794921875\n",
      "Loss: 366.68085327148435\n",
      "Loss: 365.7282409667969\n",
      "Loss: 362.71446533203124\n",
      "Loss: 352.94830322265625\n",
      "Loss: 357.04938964843745\n",
      "Loss: 357.0078186035156\n",
      "Loss: 361.8667663574219\n",
      "Loss: 363.67621459960935\n",
      "Loss: 362.24852294921874\n",
      "Loss: 359.20644531249997\n",
      "Loss: 361.3682312011719\n",
      "Loss: 361.0839172363281\n",
      "Loss: 361.9190673828125\n",
      "Loss: 362.83721313476565\n",
      "Loss: 362.693115234375\n",
      "Loss: 360.1035827636719\n",
      "Loss: 364.7436401367188\n",
      "Loss: 366.03690795898433\n",
      "Loss: 368.6639770507812\n",
      "Loss: 364.69597167968755\n",
      "Loss: 363.8517822265625\n",
      "Loss: 368.79213256835936\n",
      "Loss: 370.0592407226562\n",
      "Loss: 370.95927734374993\n",
      "Loss: 374.1729370117188\n",
      "Loss: 367.67666015625\n",
      "Loss: 370.25494995117185\n",
      "Loss: 371.89146728515624\n",
      "Loss: 370.91632080078125\n",
      "Loss: 369.201171875\n",
      "Loss: 371.7222412109375\n",
      "Loss: 361.3870788574219\n",
      "Loss: 368.02952270507814\n",
      "Loss: 365.86761474609375\n",
      "Loss: 363.5320007324219\n",
      "Loss: 364.09228515625\n",
      "Loss: 368.4398559570313\n",
      "Loss: 369.20814819335936\n",
      "Loss: 360.71193847656247\n",
      "Loss: 368.92509155273433\n",
      "Loss: 361.2202087402344\n",
      "Loss: 363.5616638183594\n",
      "Loss: 364.8415161132812\n",
      "Loss: 368.30536499023435\n",
      "Loss: 357.94026489257817\n",
      "Loss: 362.1331909179687\n",
      "Loss: 368.88317260742184\n",
      "Loss: 366.60386352539064\n",
      "Loss: 365.00517578125005\n",
      "Loss: 363.6833801269531\n",
      "Loss: 368.234228515625\n",
      "Loss: 366.88530883789065\n",
      "Loss: 367.09807128906243\n",
      "Loss: 365.91984252929694\n",
      "Loss: 370.73963623046876\n",
      "Loss: 370.716064453125\n",
      "Loss: 370.0217529296875\n",
      "Loss: 369.01784667968747\n",
      "Loss: 375.37095336914064\n",
      "Loss: 378.0222534179687\n",
      "Loss: 371.24552612304683\n",
      "Loss: 372.4006652832031\n",
      "Loss: 369.72355346679694\n",
      "Loss: 367.9801025390625\n",
      "Loss: 372.5142822265625\n",
      "Loss: 367.32526245117185\n",
      "Loss: 367.16164550781247\n",
      "Loss: 362.93534545898433\n",
      "Loss: 364.1213623046875\n",
      "Loss: 370.41232910156253\n",
      "Loss: 366.32292480468755\n",
      "Loss: 367.2570251464844\n",
      "Loss: 363.24829711914066\n",
      "Loss: 367.78980102539066\n",
      "Loss: 365.20220947265625\n",
      "Loss: 368.2991271972656\n",
      "Loss: 363.9483337402344\n",
      "Loss: 365.26117553710935\n",
      "Loss: 377.0764343261719\n",
      "Loss: 372.8797546386719\n",
      "Loss: 366.0618896484375\n",
      "Loss: 375.8448852539062\n",
      "Loss: 373.1017517089844\n",
      "Loss: 369.56796875000003\n",
      "Loss: 370.5354125976562\n",
      "Loss: 373.0745910644531\n",
      "Loss: 368.42991333007814\n",
      "Loss: 380.5486694335938\n",
      "Loss: 367.65003662109376\n",
      "Loss: 375.2385620117187\n",
      "Loss: 374.92604370117186\n",
      "Loss: 373.8110412597656\n",
      "Loss: 374.7492736816406\n",
      "Loss: 377.02045288085935\n",
      "Loss: 377.67734375\n",
      "Loss: 373.22734375\n",
      "Loss: 374.12557983398443\n",
      "Loss: 374.6798583984375\n",
      "Loss: 370.6807861328125\n",
      "Loss: 366.07908325195314\n",
      "Loss: 370.9676696777344\n",
      "Loss: 369.12673950195307\n",
      "Loss: 372.44981689453124\n",
      "Loss: 376.46892700195315\n",
      "Loss: 379.0843139648438\n",
      "Loss: 369.48366699218747\n",
      "Loss: 379.05617675781247\n",
      "Loss: 378.49959106445317\n",
      "Loss: 374.972412109375\n",
      "Loss: 369.9735168457031\n",
      "Loss: 368.9560241699219\n",
      "Loss: 373.71143798828126\n",
      "Loss: 373.88984375\n",
      "Loss: 371.61749877929685\n",
      "Loss: 379.777490234375\n",
      "Loss: 366.3709350585938\n",
      "Loss: 369.00086059570316\n",
      "Loss: 370.065380859375\n",
      "Loss: 375.987939453125\n",
      "Loss: 379.0654968261719\n",
      "Loss: 377.9365539550781\n",
      "Loss: 378.4818603515625\n",
      "Loss: 373.34398193359374\n",
      "Loss: 376.1297485351563\n",
      "Loss: 369.0164367675781\n",
      "Loss: 374.9691467285156\n",
      "Loss: 376.2967712402343\n",
      "Loss: 371.99737548828125\n",
      "Loss: 368.1061096191406\n",
      "Loss: 371.96624755859375\n",
      "Loss: 376.8344787597656\n",
      "Loss: 373.8382873535156\n",
      "Loss: 373.991552734375\n",
      "Loss: 372.1171752929688\n",
      "Loss: 376.2135559082031\n",
      "Loss: 378.7771118164063\n",
      "Loss: 370.9521057128906\n",
      "Loss: 365.3397399902344\n",
      "Loss: 361.7374633789062\n",
      "Loss: 376.64398193359375\n",
      "Loss: 374.8511047363281\n",
      "Loss: 374.18768310546875\n",
      "Loss: 369.8568298339844\n",
      "Loss: 371.67626342773434\n",
      "Loss: 370.02446289062505\n",
      "Loss: 373.0624633789063\n",
      "Loss: 367.26384277343743\n",
      "Loss: 376.86004028320315\n",
      "Loss: 371.5057250976563\n",
      "Loss: 375.48486328125\n",
      "Loss: 368.733154296875\n",
      "Loss: 369.7813903808594\n",
      "Loss: 375.14363403320317\n",
      "Loss: 373.0844604492188\n",
      "Loss: 374.80597534179685\n",
      "Loss: 369.0132202148438\n",
      "Loss: 372.3372741699219\n",
      "Loss: 370.25145263671874\n",
      "Loss: 368.7535888671875\n",
      "Loss: 370.06718139648444\n",
      "Loss: 371.5503784179688\n",
      "Loss: 367.66856079101564\n",
      "Loss: 361.9881103515625\n",
      "Loss: 373.3182495117188\n",
      "Loss: 370.7713806152344\n",
      "Loss: 376.4184875488281\n",
      "Loss: 364.130615234375\n",
      "Loss: 366.6825012207031\n",
      "Loss: 370.7228820800782\n",
      "Loss: 369.8715026855469\n",
      "Loss: 371.9269836425781\n",
      "Loss: 370.6987121582031\n",
      "Loss: 371.84169311523436\n",
      "Loss: 373.6166931152344\n",
      "Loss: 380.2795471191406\n",
      "Loss: 374.70989379882815\n",
      "Loss: 371.65155029296875\n",
      "Loss: 378.0551818847656\n",
      "Loss: 368.9628479003906\n",
      "Loss: 369.54729614257815\n",
      "Loss: 370.41069946289065\n",
      "Loss: 376.7592834472656\n",
      "Loss: 375.69403076171875\n",
      "Loss: 369.015234375\n",
      "Loss: 371.64129028320315\n",
      "Loss: 374.89990234375\n",
      "Loss: 373.6965698242187\n",
      "Loss: 375.18803710937505\n",
      "Loss: 367.7145202636719\n",
      "Loss: 371.9082092285156\n",
      "Loss: 375.07124633789067\n",
      "Loss: 378.7217712402344\n",
      "Loss: 376.2699890136718\n",
      "Loss: 372.173583984375\n",
      "Loss: 374.4360717773437\n",
      "Loss: 379.5514892578125\n",
      "Loss: 378.7219421386719\n",
      "Loss: 371.73982543945306\n",
      "Loss: 376.09613647460935\n",
      "Loss: 377.6027404785156\n",
      "Loss: 370.56927490234375\n",
      "Loss: 374.9451782226563\n",
      "Loss: 371.21913452148436\n",
      "Loss: 370.6658508300781\n",
      "Loss: 369.88886108398435\n",
      "Loss: 367.3917724609375\n",
      "Loss: 369.56503906250003\n",
      "Loss: 374.41771240234374\n",
      "Loss: 361.8036254882812\n",
      "Loss: 368.5133361816406\n",
      "Loss: 369.57775878906256\n",
      "Loss: 372.8169921875\n",
      "Loss: 366.94843139648435\n",
      "Loss: 370.6849182128907\n",
      "Loss: 363.6736694335938\n",
      "Loss: 365.3046020507812\n",
      "Loss: 368.78226928710933\n",
      "Loss: 373.24082641601564\n",
      "Loss: 372.2347106933594\n",
      "Loss: 375.9358154296875\n",
      "Loss: 365.32941284179685\n"
     ]
    }
   ],
   "source": [
    "before_online_results, after_online_results = test_forgetting_odadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'r': [0.7820833333333334, 0.72125],\n",
       "  'r_ot': [0.98125, 0.9908333333333333]},\n",
       " 'rbf': {'r': [0.8270833333333333, 0.8691666666666666], 'r_ot': [1.0, 1.0]},\n",
       " 'RF': {'r': [0.7520833333333333, 0.7433333333333333],\n",
       "  'r_ot': [0.9861666666666669, 0.9935833333333333]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_online_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'r': [0.6770833333333334, 0.7508333333333334],\n",
       "  'r_ot': [0.9825000000000002, 0.9870833333333333]},\n",
       " 'rbf': {'r': [0.74125, 0.8533333333333334], 'r_ot': [1.0, 1.0]},\n",
       " 'RF': {'r': [0.8454166666666667, 0.7875], 'r_ot': [0.9850416666666666, 0.99]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_online_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
