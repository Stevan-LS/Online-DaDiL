{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Tests_ODaDiL_CWRU import test_dadil, test_odadil, test_forgetting_odadil\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_datasets = []\n",
    "for i in range(1, 11):\n",
    "    dataset = np.load(f'data/toy_non_linear_100d_dataset_{i}.npy')\n",
    "    list_of_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'C'\n",
    "with open(os.path.join('data', 'mlp_fts_256_target_{}.pkl'.format(target)), 'rb') as f:\n",
    "        dataset = pickle.loads(f.read())\n",
    "\n",
    "Xs, ys = [], []\n",
    "d = None\n",
    "keys = list(dataset.keys())\n",
    "for i in [0, 2]:\n",
    "    features = dataset[keys[i]]['Features']\n",
    "    labels = dataset[keys[i]]['Labels'].argmax(dim=1)\n",
    "    domain = i*np.ones((features.shape[0], 1))\n",
    "    Xs.append(features.float())\n",
    "    ys.append(labels.float())\n",
    "    if d is None:\n",
    "        d = domain\n",
    "    else:\n",
    "        d = np.concatenate([d, domain], axis=0)\n",
    "\n",
    "Xt = dataset[target]['fold 0']['Train']['Features'].float()\n",
    "yt = dataset[target]['fold 0']['Train']['Labels'].float().argmax(dim=1)\n",
    "\n",
    "Xt_test = dataset[target]['fold 0']['Test']['Features'].float()\n",
    "yt_test = dataset[target]['fold 0']['Test']['Labels'].float().argmax(dim=1)\n",
    "d = np.concatenate([d, 2*np.ones((Xt.shape[0], 1))], axis=0)\n",
    "\n",
    "n_domains = int(np.max(d)) + 1\n",
    "n_features = Xt.shape[1]\n",
    "n_classes = int(np.max(yt.numpy())) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "batch_size = 200\n",
    "n_atoms = 3\n",
    "n_classes = 10\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 7410.03173828125\n",
      "It 2/100, Loss: 5434.06708984375\n",
      "It 3/100, Loss: 3959.6238281250003\n",
      "It 4/100, Loss: 3018.6269531250005\n",
      "It 5/100, Loss: 2338.546142578125\n",
      "It 6/100, Loss: 1850.917724609375\n",
      "It 7/100, Loss: 1472.7577880859374\n",
      "It 8/100, Loss: 1172.9741455078126\n",
      "It 9/100, Loss: 951.0959228515626\n",
      "It 10/100, Loss: 774.662841796875\n",
      "It 11/100, Loss: 645.2108642578124\n",
      "It 12/100, Loss: 549.780419921875\n",
      "It 13/100, Loss: 479.5917114257812\n",
      "It 14/100, Loss: 420.4764526367187\n",
      "It 15/100, Loss: 388.74168090820314\n",
      "It 16/100, Loss: 339.52911987304685\n",
      "It 17/100, Loss: 316.65272216796876\n",
      "It 18/100, Loss: 288.9431945800781\n",
      "It 19/100, Loss: 259.4006652832031\n",
      "It 20/100, Loss: 261.90569763183595\n",
      "It 21/100, Loss: 246.58533935546876\n",
      "It 22/100, Loss: 225.20755920410159\n",
      "It 23/100, Loss: 206.54473266601562\n",
      "It 24/100, Loss: 207.5506866455078\n",
      "It 25/100, Loss: 175.09521484375003\n",
      "It 26/100, Loss: 163.00850524902341\n",
      "It 27/100, Loss: 138.21900482177733\n",
      "It 28/100, Loss: 113.83731994628907\n",
      "It 29/100, Loss: 107.89107818603516\n",
      "It 30/100, Loss: 104.87378540039063\n",
      "It 31/100, Loss: 93.09907684326173\n",
      "It 32/100, Loss: 77.32795562744141\n",
      "It 33/100, Loss: 77.02442932128906\n",
      "It 34/100, Loss: 65.25098876953125\n",
      "It 35/100, Loss: 65.16386947631835\n",
      "It 36/100, Loss: 60.97023620605469\n",
      "It 37/100, Loss: 61.901734924316415\n",
      "It 38/100, Loss: 56.73006134033203\n",
      "It 39/100, Loss: 67.77935180664062\n",
      "It 40/100, Loss: 59.535193634033206\n",
      "It 41/100, Loss: 52.32368850708008\n",
      "It 42/100, Loss: 50.737762451171875\n",
      "It 43/100, Loss: 53.968465423583986\n",
      "It 44/100, Loss: 58.5134880065918\n",
      "It 45/100, Loss: 60.534143066406244\n",
      "It 46/100, Loss: 55.23175964355468\n",
      "It 47/100, Loss: 61.8449722290039\n",
      "It 48/100, Loss: 56.77821731567383\n",
      "It 49/100, Loss: 52.817796325683595\n",
      "It 50/100, Loss: 51.52668838500976\n",
      "It 51/100, Loss: 47.72248840332031\n",
      "It 52/100, Loss: 54.34677047729492\n",
      "It 53/100, Loss: 54.088627624511716\n",
      "It 54/100, Loss: 45.42642364501953\n",
      "It 55/100, Loss: 57.980690002441406\n",
      "It 56/100, Loss: 50.97141952514648\n",
      "It 57/100, Loss: 45.908791351318364\n",
      "It 58/100, Loss: 52.73815765380859\n",
      "It 59/100, Loss: 54.29147033691406\n",
      "It 60/100, Loss: 52.51427383422851\n",
      "It 61/100, Loss: 52.26637725830078\n",
      "It 62/100, Loss: 49.33088607788086\n",
      "It 63/100, Loss: 45.679744720458984\n",
      "It 64/100, Loss: 47.252377319335935\n",
      "It 65/100, Loss: 43.52855606079102\n",
      "It 66/100, Loss: 45.33979873657227\n",
      "It 67/100, Loss: 41.27960739135742\n",
      "It 68/100, Loss: 48.432865142822266\n",
      "It 69/100, Loss: 58.223148345947266\n",
      "It 70/100, Loss: 49.53700332641601\n",
      "It 71/100, Loss: 43.6997055053711\n",
      "It 72/100, Loss: 47.96371002197265\n",
      "It 73/100, Loss: 48.05487213134765\n",
      "It 74/100, Loss: 48.265539550781256\n",
      "It 75/100, Loss: 42.80936241149902\n",
      "It 76/100, Loss: 37.989575958251955\n",
      "It 77/100, Loss: 49.30891723632812\n",
      "It 78/100, Loss: 44.96348571777344\n",
      "It 79/100, Loss: 44.715222930908205\n",
      "It 80/100, Loss: 41.96371955871582\n",
      "It 81/100, Loss: 37.48788909912109\n",
      "It 82/100, Loss: 42.904814147949224\n",
      "It 83/100, Loss: 38.86002807617187\n",
      "It 84/100, Loss: 45.53633422851562\n",
      "It 85/100, Loss: 37.35140380859375\n",
      "It 86/100, Loss: 39.91182250976563\n",
      "It 87/100, Loss: 44.00852737426758\n",
      "It 88/100, Loss: 39.81422882080078\n",
      "It 89/100, Loss: 47.153218841552736\n",
      "It 90/100, Loss: 47.536834716796875\n",
      "It 91/100, Loss: 40.2650146484375\n",
      "It 92/100, Loss: 40.53768386840821\n",
      "It 93/100, Loss: 36.268463134765625\n",
      "It 94/100, Loss: 38.73834571838379\n",
      "It 95/100, Loss: 40.51928482055664\n",
      "It 96/100, Loss: 37.61046676635742\n",
      "It 97/100, Loss: 39.55864791870117\n",
      "It 98/100, Loss: 37.89312057495117\n",
      "It 99/100, Loss: 60.11908569335938\n",
      "It 100/100, Loss: 45.116564178466795\n"
     ]
    }
   ],
   "source": [
    "results = test_dadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'wda': 0.7745833333333333,\n",
       "  'e': 0.31083333333333335,\n",
       "  'e_ot': 0.8665416666666668,\n",
       "  'r': 0.8729166666666667,\n",
       "  'r_ot': 0.9245833333333333},\n",
       " 'rbf': {'wda': 0.78875,\n",
       "  'e': 0.70875,\n",
       "  'e_ot': 0.983875,\n",
       "  'r': 0.9991666666666666,\n",
       "  'r_ot': 0.9991666666666668},\n",
       " 'RF': {'wda': 0.78,\n",
       "  'e': 0.5916666666666667,\n",
       "  'e_ot': 0.8619166666666667,\n",
       "  'r': 0.9370833333333334,\n",
       "  'r_ot': 0.8642916666666667}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 6293.8759765625\n",
      "It 2/100, Loss: 4682.848535156249\n",
      "It 3/100, Loss: 3421.94736328125\n",
      "It 4/100, Loss: 2578.3666503906247\n",
      "It 5/100, Loss: 2062.232421875\n",
      "It 6/100, Loss: 1620.34677734375\n",
      "It 7/100, Loss: 1305.6499999999999\n",
      "It 8/100, Loss: 1061.6853759765627\n",
      "It 9/100, Loss: 859.5136474609375\n",
      "It 10/100, Loss: 696.1669067382812\n",
      "It 11/100, Loss: 569.10986328125\n",
      "It 12/100, Loss: 471.69270019531245\n",
      "It 13/100, Loss: 388.2576110839844\n",
      "It 14/100, Loss: 325.369189453125\n",
      "It 15/100, Loss: 272.4176239013672\n",
      "It 16/100, Loss: 230.7188262939453\n",
      "It 17/100, Loss: 195.6434356689453\n",
      "It 18/100, Loss: 166.90877685546874\n",
      "It 19/100, Loss: 143.29924926757812\n",
      "It 20/100, Loss: 123.6897201538086\n",
      "It 21/100, Loss: 109.82488250732422\n",
      "It 22/100, Loss: 96.57325744628906\n",
      "It 23/100, Loss: 85.04256591796874\n",
      "It 24/100, Loss: 77.36022491455078\n",
      "It 25/100, Loss: 69.84381866455078\n",
      "It 26/100, Loss: 63.04319915771484\n",
      "It 27/100, Loss: 57.51739349365235\n",
      "It 28/100, Loss: 52.16697235107422\n",
      "It 29/100, Loss: 48.47245635986328\n",
      "It 30/100, Loss: 45.91000671386719\n",
      "It 31/100, Loss: 42.49046173095704\n",
      "It 32/100, Loss: 39.44667282104492\n",
      "It 33/100, Loss: 37.68490219116211\n",
      "It 34/100, Loss: 34.98006973266602\n",
      "It 35/100, Loss: 33.71126861572266\n",
      "It 36/100, Loss: 32.31121063232422\n",
      "It 37/100, Loss: 31.511260223388675\n",
      "It 38/100, Loss: 30.341242218017577\n",
      "It 39/100, Loss: 28.953029251098634\n",
      "It 40/100, Loss: 27.82224769592285\n",
      "It 41/100, Loss: 27.35529556274414\n",
      "It 42/100, Loss: 26.724758911132813\n",
      "It 43/100, Loss: 26.23380813598633\n",
      "It 44/100, Loss: 25.873257827758792\n",
      "It 45/100, Loss: 24.542714309692382\n",
      "It 46/100, Loss: 25.702023696899417\n",
      "It 47/100, Loss: 24.33607406616211\n",
      "It 48/100, Loss: 23.848889160156254\n",
      "It 49/100, Loss: 23.47748565673828\n",
      "It 50/100, Loss: 23.017869567871095\n",
      "It 51/100, Loss: 22.887099456787112\n",
      "It 52/100, Loss: 22.81385536193848\n",
      "It 53/100, Loss: 21.842547988891603\n",
      "It 54/100, Loss: 22.153354644775387\n",
      "It 55/100, Loss: 22.44486351013183\n",
      "It 56/100, Loss: 21.866172790527344\n",
      "It 57/100, Loss: 21.357247924804685\n",
      "It 58/100, Loss: 20.485540771484374\n",
      "It 59/100, Loss: 20.60331115722656\n",
      "It 60/100, Loss: 20.38153991699219\n",
      "It 61/100, Loss: 20.13151626586914\n",
      "It 62/100, Loss: 19.16455039978027\n",
      "It 63/100, Loss: 19.069623947143555\n",
      "It 64/100, Loss: 18.691059875488282\n",
      "It 65/100, Loss: 18.18127403259277\n",
      "It 66/100, Loss: 17.284082412719727\n",
      "It 67/100, Loss: 16.55358200073242\n",
      "It 68/100, Loss: 16.405572128295898\n",
      "It 69/100, Loss: 15.721343040466309\n",
      "It 70/100, Loss: 14.653496742248537\n",
      "It 71/100, Loss: 14.257590866088869\n",
      "It 72/100, Loss: 13.096854209899902\n",
      "It 73/100, Loss: 12.696994972229005\n",
      "It 74/100, Loss: 12.083976745605469\n",
      "It 75/100, Loss: 11.652109909057618\n",
      "It 76/100, Loss: 12.740934371948244\n",
      "It 77/100, Loss: 23.130681037902832\n",
      "It 78/100, Loss: 14.197865295410155\n",
      "It 79/100, Loss: 11.049977874755859\n",
      "It 80/100, Loss: 10.82091865539551\n",
      "It 81/100, Loss: 10.699757957458495\n",
      "It 82/100, Loss: 10.513055038452148\n",
      "It 83/100, Loss: 13.326996231079102\n",
      "It 84/100, Loss: 10.145439147949219\n",
      "It 85/100, Loss: 10.33146686553955\n",
      "It 86/100, Loss: 9.97198143005371\n",
      "It 87/100, Loss: 10.340658950805665\n",
      "It 88/100, Loss: 9.970607948303222\n",
      "It 89/100, Loss: 11.197222709655762\n",
      "It 90/100, Loss: 14.203805160522462\n",
      "It 91/100, Loss: 9.649737930297851\n",
      "It 92/100, Loss: 9.708036041259765\n",
      "It 93/100, Loss: 9.874695205688477\n",
      "It 94/100, Loss: 9.479807090759278\n",
      "It 95/100, Loss: 9.616747093200683\n",
      "It 96/100, Loss: 9.615435600280762\n",
      "It 97/100, Loss: 9.444824028015137\n",
      "It 98/100, Loss: 9.537777709960938\n",
      "It 99/100, Loss: 9.600807571411133\n",
      "It 100/100, Loss: 8.945318984985352\n",
      "Loss: 275.4000183105469\n",
      "Loss: 259.58652954101564\n",
      "Loss: 261.91884765624997\n",
      "Loss: 274.20151977539064\n",
      "Loss: 266.953173828125\n",
      "Loss: 266.46530761718753\n",
      "Loss: 266.7726501464844\n",
      "Loss: 264.0885101318359\n",
      "Loss: 268.75770263671876\n",
      "Loss: 274.6601257324219\n",
      "Loss: 282.0658325195313\n",
      "Loss: 284.1360656738281\n",
      "Loss: 285.6827331542969\n",
      "Loss: 285.91601562499994\n",
      "Loss: 289.3071716308594\n",
      "Loss: 298.4242797851563\n",
      "Loss: 297.6896484375\n",
      "Loss: 300.027880859375\n",
      "Loss: 303.07603759765624\n",
      "Loss: 307.6475769042969\n",
      "Loss: 314.95460815429686\n",
      "Loss: 328.7583068847656\n",
      "Loss: 319.9444946289062\n",
      "Loss: 320.6001953125\n",
      "Loss: 321.9566589355469\n",
      "Loss: 329.7927368164062\n",
      "Loss: 324.677099609375\n",
      "Loss: 332.75771484375\n",
      "Loss: 332.50890502929684\n",
      "Loss: 336.3545043945313\n",
      "Loss: 330.6890502929687\n",
      "Loss: 333.99139404296875\n",
      "Loss: 336.2122436523438\n",
      "Loss: 336.96248168945317\n",
      "Loss: 343.4822082519531\n",
      "Loss: 345.19963989257815\n",
      "Loss: 339.2476135253906\n",
      "Loss: 342.9386901855469\n",
      "Loss: 347.30463867187495\n",
      "Loss: 348.6779602050781\n",
      "Loss: 353.4496276855469\n",
      "Loss: 357.637158203125\n",
      "Loss: 350.9435119628907\n",
      "Loss: 358.6976135253906\n",
      "Loss: 359.78633422851556\n",
      "Loss: 354.7598815917969\n",
      "Loss: 359.3155395507813\n",
      "Loss: 351.6647216796875\n",
      "Loss: 356.2243530273437\n",
      "Loss: 356.6559753417969\n",
      "Loss: 355.2562683105469\n",
      "Loss: 350.46298828125\n",
      "Loss: 354.15822143554686\n",
      "Loss: 354.78798828125\n",
      "Loss: 353.3110290527344\n",
      "Loss: 364.9549926757812\n",
      "Loss: 358.39392700195316\n",
      "Loss: 363.3487915039062\n",
      "Loss: 372.9949768066406\n",
      "Loss: 364.96867065429683\n",
      "Loss: 361.0576232910156\n",
      "Loss: 363.1246398925781\n",
      "Loss: 351.6577026367188\n",
      "Loss: 368.81718750000005\n",
      "Loss: 368.3012634277344\n",
      "Loss: 362.54953613281253\n",
      "Loss: 365.68626098632814\n",
      "Loss: 369.3597900390625\n",
      "Loss: 367.523193359375\n",
      "Loss: 370.534423828125\n",
      "Loss: 367.1685852050781\n",
      "Loss: 361.70643310546876\n",
      "Loss: 369.3502258300781\n",
      "Loss: 368.1944885253906\n",
      "Loss: 366.0688598632812\n",
      "Loss: 368.6284118652344\n",
      "Loss: 362.6524841308593\n",
      "Loss: 364.76413574218753\n",
      "Loss: 365.07976074218755\n",
      "Loss: 360.60444946289067\n",
      "Loss: 364.47916870117183\n",
      "Loss: 363.8492553710937\n",
      "Loss: 365.2705017089844\n",
      "Loss: 372.4391845703125\n",
      "Loss: 366.6585632324219\n",
      "Loss: 369.9180419921875\n",
      "Loss: 369.39508056640625\n",
      "Loss: 368.9304260253906\n",
      "Loss: 367.9216064453125\n",
      "Loss: 366.8311218261719\n",
      "Loss: 367.5578247070313\n",
      "Loss: 366.4344848632812\n",
      "Loss: 368.7303771972656\n",
      "Loss: 362.09632568359376\n",
      "Loss: 368.6509582519531\n",
      "Loss: 361.8096069335937\n",
      "Loss: 373.284033203125\n",
      "Loss: 370.57102661132814\n",
      "Loss: 366.09966430664065\n",
      "Loss: 371.01760253906247\n",
      "Loss: 369.25625\n",
      "Loss: 369.95903320312505\n",
      "Loss: 363.26903076171874\n",
      "Loss: 370.40739135742183\n",
      "Loss: 365.641748046875\n",
      "Loss: 355.21937866210936\n",
      "Loss: 362.0754638671875\n",
      "Loss: 360.93714599609376\n",
      "Loss: 363.25822143554683\n",
      "Loss: 366.53172607421874\n",
      "Loss: 368.6294616699219\n",
      "Loss: 367.8232360839844\n",
      "Loss: 365.6952087402344\n",
      "Loss: 362.52081909179685\n",
      "Loss: 366.54856567382814\n",
      "Loss: 367.31181640624993\n",
      "Loss: 370.6390747070313\n",
      "Loss: 364.26234741210936\n",
      "Loss: 361.92034912109375\n",
      "Loss: 361.8214172363281\n",
      "Loss: 359.0497375488281\n",
      "Loss: 370.5470153808593\n",
      "Loss: 370.909033203125\n",
      "Loss: 364.2239868164063\n",
      "Loss: 364.9814270019531\n",
      "Loss: 368.6826904296875\n",
      "Loss: 369.89697265625\n",
      "Loss: 375.29118652343755\n",
      "Loss: 368.74558105468753\n",
      "Loss: 371.31781616210935\n",
      "Loss: 368.19361572265626\n",
      "Loss: 373.7491821289062\n",
      "Loss: 368.4618225097656\n",
      "Loss: 377.1316833496094\n",
      "Loss: 377.94049682617185\n",
      "Loss: 370.50493774414065\n",
      "Loss: 368.0912231445313\n",
      "Loss: 368.3986877441406\n",
      "Loss: 372.7495422363281\n",
      "Loss: 369.8624572753906\n",
      "Loss: 365.62119140625\n",
      "Loss: 374.99253540039064\n",
      "Loss: 370.3677551269531\n",
      "Loss: 367.85193481445305\n",
      "Loss: 371.8070861816406\n",
      "Loss: 373.3180358886719\n",
      "Loss: 372.7523254394531\n",
      "Loss: 370.8772155761719\n",
      "Loss: 371.65048828125\n",
      "Loss: 373.2065795898438\n",
      "Loss: 372.81314086914057\n",
      "Loss: 362.58783569335935\n",
      "Loss: 368.61705932617184\n",
      "Loss: 379.6484436035156\n",
      "Loss: 368.3200988769531\n",
      "Loss: 374.42063598632814\n",
      "Loss: 374.6203735351562\n",
      "Loss: 367.12340087890624\n",
      "Loss: 366.8138061523438\n",
      "Loss: 368.34259643554685\n",
      "Loss: 374.703125\n",
      "Loss: 369.4330749511719\n",
      "Loss: 371.7173583984375\n",
      "Loss: 375.56723632812503\n",
      "Loss: 372.0124328613282\n",
      "Loss: 368.0366088867188\n",
      "Loss: 370.825341796875\n",
      "Loss: 370.1369934082031\n",
      "Loss: 375.6517639160156\n",
      "Loss: 368.0192932128906\n",
      "Loss: 373.946630859375\n",
      "Loss: 379.1590209960938\n",
      "Loss: 368.8388549804688\n",
      "Loss: 377.1936401367187\n",
      "Loss: 370.8411743164063\n",
      "Loss: 369.89746704101566\n",
      "Loss: 366.47490234375\n",
      "Loss: 364.1015258789063\n",
      "Loss: 367.5355224609375\n",
      "Loss: 365.34217529296876\n",
      "Loss: 368.0406494140625\n",
      "Loss: 372.4950805664062\n",
      "Loss: 375.29108886718745\n",
      "Loss: 370.61291503906256\n",
      "Loss: 369.0630615234375\n",
      "Loss: 373.8781005859375\n",
      "Loss: 366.5231567382813\n",
      "Loss: 366.2027099609375\n",
      "Loss: 365.37534790039064\n",
      "Loss: 368.8525146484375\n",
      "Loss: 370.37947387695317\n",
      "Loss: 372.93193359375005\n",
      "Loss: 370.2444030761719\n",
      "Loss: 360.0855712890625\n",
      "Loss: 366.4549621582031\n",
      "Loss: 367.1651489257812\n",
      "Loss: 370.69857788085943\n",
      "Loss: 366.87509765625003\n",
      "Loss: 363.998486328125\n",
      "Loss: 368.8725402832031\n",
      "Loss: 362.618994140625\n",
      "Loss: 367.8069641113281\n",
      "Loss: 363.3172668457031\n",
      "Loss: 364.3659240722656\n",
      "Loss: 367.913720703125\n",
      "Loss: 368.7432556152344\n",
      "Loss: 364.02724609375\n",
      "Loss: 369.1690246582031\n",
      "Loss: 363.2549743652344\n",
      "Loss: 365.3210876464844\n",
      "Loss: 364.9841613769531\n",
      "Loss: 370.0338989257813\n",
      "Loss: 366.9919006347656\n",
      "Loss: 367.4641174316406\n",
      "Loss: 370.59038696289065\n",
      "Loss: 369.3133666992187\n",
      "Loss: 361.0005004882812\n",
      "Loss: 364.5964416503906\n",
      "Loss: 364.7132263183593\n",
      "Loss: 366.089013671875\n",
      "Loss: 366.13666992187495\n",
      "Loss: 366.9509948730469\n",
      "Loss: 366.24183959960936\n",
      "Loss: 369.53828125\n",
      "Loss: 363.66400756835935\n",
      "Loss: 369.7730224609375\n",
      "Loss: 366.03264770507815\n",
      "Loss: 367.7403686523437\n",
      "Loss: 367.0443420410157\n",
      "Loss: 360.8942626953125\n",
      "Loss: 371.58587646484375\n",
      "Loss: 364.1510375976562\n",
      "Loss: 372.0830749511719\n",
      "Loss: 364.577783203125\n",
      "Loss: 366.2019287109375\n",
      "Loss: 369.86339111328124\n",
      "Loss: 367.47749633789067\n",
      "Loss: 367.9630676269531\n",
      "Loss: 368.7442565917969\n",
      "Loss: 370.93745727539067\n",
      "Loss: 372.76494140625\n",
      "Loss: 370.83305053710933\n",
      "Loss: 370.98436889648434\n",
      "Loss: 372.1023498535156\n",
      "Loss: 370.2256225585937\n",
      "Loss: 369.1595092773438\n",
      "Loss: 377.8404907226562\n",
      "Loss: 369.23156127929684\n",
      "Loss: 364.6277954101563\n",
      "Loss: 363.2290344238281\n",
      "Loss: 369.6423461914062\n",
      "Loss: 367.1398193359375\n",
      "Loss: 371.7737915039063\n",
      "Loss: 373.07861328125\n",
      "Loss: 371.3917114257813\n",
      "Loss: 373.13377075195314\n",
      "Loss: 366.9676452636719\n",
      "Loss: 375.0782165527344\n",
      "Loss: 371.493115234375\n",
      "Loss: 374.59611206054683\n",
      "Loss: 374.61903076171876\n",
      "Loss: 370.39849243164065\n",
      "Loss: 367.2731994628906\n",
      "Loss: 373.3340698242188\n",
      "Loss: 374.32653198242184\n",
      "Loss: 376.6292602539063\n",
      "Loss: 366.83391723632815\n",
      "Loss: 376.0475219726562\n",
      "Loss: 371.87689819335935\n",
      "Loss: 375.2602294921874\n",
      "Loss: 372.27674560546876\n",
      "Loss: 373.6627197265625\n",
      "Loss: 379.2659973144531\n",
      "Loss: 375.0364807128906\n",
      "Loss: 372.28325805664065\n",
      "Loss: 377.9282287597656\n",
      "Loss: 367.6766845703125\n",
      "Loss: 380.53765869140625\n",
      "Loss: 370.5099731445313\n"
     ]
    }
   ],
   "source": [
    "results_odadil = test_odadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'wda': 0.7745833333333333,\n",
       "  'e': 0.8008333333333333,\n",
       "  'e_ot': 0.9652916666666667,\n",
       "  'r': 0.84625,\n",
       "  'r_ot': 0.9637499999999999},\n",
       " 'rbf': {'wda': 0.78875,\n",
       "  'e': 0.8033333333333333,\n",
       "  'e_ot': 0.9929166666666667,\n",
       "  'r': 0.7995833333333333,\n",
       "  'r_ot': 0.9870833333333333},\n",
       " 'RF': {'wda': 0.8129166666666666,\n",
       "  'e': 0.7533333333333333,\n",
       "  'e_ot': 0.9692916666666666,\n",
       "  'r': 0.7095833333333333,\n",
       "  'r_ot': 0.9678333333333333}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_odadil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 6223.84228515625\n",
      "It 2/100, Loss: 4661.515722656251\n",
      "It 3/100, Loss: 3382.2721191406254\n",
      "It 4/100, Loss: 2596.553369140625\n",
      "It 5/100, Loss: 2013.717578125\n",
      "It 6/100, Loss: 1605.8345947265625\n",
      "It 7/100, Loss: 1323.16767578125\n",
      "It 8/100, Loss: 1052.4798461914063\n",
      "It 9/100, Loss: 860.4164794921875\n",
      "It 10/100, Loss: 713.5382324218749\n",
      "It 11/100, Loss: 575.3792724609375\n",
      "It 12/100, Loss: 477.49216918945314\n",
      "It 13/100, Loss: 395.24690551757817\n",
      "It 14/100, Loss: 328.23022460937506\n",
      "It 15/100, Loss: 273.46901550292966\n",
      "It 16/100, Loss: 230.7112243652344\n",
      "It 17/100, Loss: 199.1666290283203\n",
      "It 18/100, Loss: 171.86464538574216\n",
      "It 19/100, Loss: 146.34405212402345\n",
      "It 20/100, Loss: 125.87306213378906\n",
      "It 21/100, Loss: 109.86471862792968\n",
      "It 22/100, Loss: 97.10089569091797\n",
      "It 23/100, Loss: 87.4774429321289\n",
      "It 24/100, Loss: 77.21378936767577\n",
      "It 25/100, Loss: 70.01371002197266\n",
      "It 26/100, Loss: 64.5491455078125\n",
      "It 27/100, Loss: 58.104952239990226\n",
      "It 28/100, Loss: 52.727304840087896\n",
      "It 29/100, Loss: 49.222312927246094\n",
      "It 30/100, Loss: 45.83017425537109\n",
      "It 31/100, Loss: 42.59400939941406\n",
      "It 32/100, Loss: 40.33198623657226\n",
      "It 33/100, Loss: 38.7136589050293\n",
      "It 34/100, Loss: 36.08708953857422\n",
      "It 35/100, Loss: 33.80115737915039\n",
      "It 36/100, Loss: 33.017094421386716\n",
      "It 37/100, Loss: 31.386119842529297\n",
      "It 38/100, Loss: 30.45828857421875\n",
      "It 39/100, Loss: 29.29185180664063\n",
      "It 40/100, Loss: 28.791677474975582\n",
      "It 41/100, Loss: 27.54267959594727\n",
      "It 42/100, Loss: 26.78591613769531\n",
      "It 43/100, Loss: 26.432532882690428\n",
      "It 44/100, Loss: 25.231526565551757\n",
      "It 45/100, Loss: 25.249730682373045\n",
      "It 46/100, Loss: 24.60698547363281\n",
      "It 47/100, Loss: 24.127632141113285\n",
      "It 48/100, Loss: 23.40631675720215\n",
      "It 49/100, Loss: 23.12822685241699\n",
      "It 50/100, Loss: 22.635272598266603\n",
      "It 51/100, Loss: 22.101269149780272\n",
      "It 52/100, Loss: 21.59487190246582\n",
      "It 53/100, Loss: 21.69312438964844\n",
      "It 54/100, Loss: 20.813422775268553\n",
      "It 55/100, Loss: 20.146685791015624\n",
      "It 56/100, Loss: 20.021577453613283\n",
      "It 57/100, Loss: 18.95086097717285\n",
      "It 58/100, Loss: 18.190804290771485\n",
      "It 59/100, Loss: 17.732707595825193\n",
      "It 60/100, Loss: 17.01013412475586\n",
      "It 61/100, Loss: 16.405346298217772\n",
      "It 62/100, Loss: 15.838641357421874\n",
      "It 63/100, Loss: 14.989206314086914\n",
      "It 64/100, Loss: 14.49029369354248\n",
      "It 65/100, Loss: 13.589562606811525\n",
      "It 66/100, Loss: 13.271631813049314\n",
      "It 67/100, Loss: 12.424773788452148\n",
      "It 68/100, Loss: 12.41294803619385\n",
      "It 69/100, Loss: 11.895720291137696\n",
      "It 70/100, Loss: 11.631446838378906\n",
      "It 71/100, Loss: 11.449839401245116\n",
      "It 72/100, Loss: 11.267761611938479\n",
      "It 73/100, Loss: 10.771888160705567\n",
      "It 74/100, Loss: 10.554943084716797\n",
      "It 75/100, Loss: 10.694906806945799\n",
      "It 76/100, Loss: 10.455150604248047\n",
      "It 77/100, Loss: 13.475904655456544\n",
      "It 78/100, Loss: 10.375480270385742\n",
      "It 79/100, Loss: 13.257059478759766\n",
      "It 80/100, Loss: 10.119536972045898\n",
      "It 81/100, Loss: 13.679296684265136\n",
      "It 82/100, Loss: 12.209076881408691\n",
      "It 83/100, Loss: 10.268657684326172\n",
      "It 84/100, Loss: 9.881057167053223\n",
      "It 85/100, Loss: 9.954089164733887\n",
      "It 86/100, Loss: 9.860510063171388\n",
      "It 87/100, Loss: 9.82367572784424\n",
      "It 88/100, Loss: 9.619992446899413\n",
      "It 89/100, Loss: 9.85744342803955\n",
      "It 90/100, Loss: 11.119421768188477\n",
      "It 91/100, Loss: 12.66012477874756\n",
      "It 92/100, Loss: 14.207954406738281\n",
      "It 93/100, Loss: 13.179095649719239\n",
      "It 94/100, Loss: 11.02470932006836\n",
      "It 95/100, Loss: 11.871900939941407\n",
      "It 96/100, Loss: 9.596714210510253\n",
      "It 97/100, Loss: 9.621850776672364\n",
      "It 98/100, Loss: 9.721381187438965\n",
      "It 99/100, Loss: 9.455593681335449\n",
      "It 100/100, Loss: 9.915958213806153\n",
      "Loss: 257.9605224609375\n",
      "Loss: 257.5367004394531\n",
      "Loss: 263.7227478027344\n",
      "Loss: 271.0867950439453\n",
      "Loss: 258.8880584716797\n",
      "Loss: 271.3107116699219\n",
      "Loss: 266.76511230468753\n",
      "Loss: 266.06463012695315\n",
      "Loss: 264.35704345703124\n",
      "Loss: 271.58838500976566\n",
      "Loss: 270.5977294921875\n",
      "Loss: 281.394287109375\n",
      "Loss: 278.66665039062497\n",
      "Loss: 289.45009155273436\n",
      "Loss: 287.2012573242188\n",
      "Loss: 291.20363159179686\n",
      "Loss: 301.5692077636719\n",
      "Loss: 303.19877319335933\n",
      "Loss: 305.28529663085936\n",
      "Loss: 307.28955688476566\n",
      "Loss: 313.9563842773438\n",
      "Loss: 320.1277404785156\n",
      "Loss: 321.8877014160156\n",
      "Loss: 321.01536254882814\n",
      "Loss: 326.5738098144531\n",
      "Loss: 323.0879272460938\n",
      "Loss: 327.99878540039066\n",
      "Loss: 332.40614013671876\n",
      "Loss: 333.83331298828125\n",
      "Loss: 335.25535888671874\n",
      "Loss: 338.2485412597656\n",
      "Loss: 334.9412719726563\n",
      "Loss: 343.2319152832031\n",
      "Loss: 355.2143981933594\n",
      "Loss: 341.6248046875\n",
      "Loss: 334.75370483398433\n",
      "Loss: 347.30443115234374\n",
      "Loss: 344.4927734375\n",
      "Loss: 351.53776245117183\n",
      "Loss: 350.05345458984374\n",
      "Loss: 345.5690124511719\n",
      "Loss: 345.5439147949219\n",
      "Loss: 348.2379455566406\n",
      "Loss: 360.1868774414063\n",
      "Loss: 356.8840637207031\n",
      "Loss: 351.1529174804688\n",
      "Loss: 361.84697265625\n",
      "Loss: 353.33858032226567\n",
      "Loss: 357.40830078125003\n",
      "Loss: 361.23280639648436\n",
      "Loss: 360.63911743164056\n",
      "Loss: 354.0817016601563\n",
      "Loss: 352.78399047851565\n",
      "Loss: 355.30360107421876\n",
      "Loss: 359.05776367187497\n",
      "Loss: 362.3331665039062\n",
      "Loss: 362.5473571777344\n",
      "Loss: 361.7967346191406\n",
      "Loss: 365.98796997070315\n",
      "Loss: 365.2022277832031\n",
      "Loss: 364.0790649414063\n",
      "Loss: 363.91318359375003\n",
      "Loss: 364.31287841796876\n",
      "Loss: 365.1616455078125\n",
      "Loss: 368.86146240234376\n",
      "Loss: 373.3065368652344\n",
      "Loss: 372.2612609863282\n",
      "Loss: 367.41925659179685\n",
      "Loss: 373.23798828125\n",
      "Loss: 369.2800659179687\n",
      "Loss: 360.80341796875\n",
      "Loss: 368.27548828125\n",
      "Loss: 364.6050720214844\n",
      "Loss: 367.7916625976562\n",
      "Loss: 362.9742431640625\n",
      "Loss: 363.83773803710943\n",
      "Loss: 363.5852355957031\n",
      "Loss: 361.235595703125\n",
      "Loss: 364.27666625976565\n",
      "Loss: 366.1598876953125\n",
      "Loss: 361.4953979492187\n",
      "Loss: 365.66064453125\n",
      "Loss: 361.7595642089844\n",
      "Loss: 366.3950439453125\n",
      "Loss: 367.3465942382813\n",
      "Loss: 366.4234130859375\n",
      "Loss: 373.66068725585933\n",
      "Loss: 365.8981079101562\n",
      "Loss: 375.6791137695313\n",
      "Loss: 362.0169677734375\n",
      "Loss: 365.5136901855469\n",
      "Loss: 368.31737670898434\n",
      "Loss: 368.80645141601565\n",
      "Loss: 366.56101684570314\n",
      "Loss: 365.692236328125\n",
      "Loss: 364.8847045898438\n",
      "Loss: 360.80047607421875\n",
      "Loss: 364.69232177734375\n",
      "Loss: 367.38249511718755\n",
      "Loss: 369.39795532226566\n",
      "Loss: 375.1324768066406\n",
      "Loss: 370.2345520019531\n",
      "Loss: 361.73886108398443\n",
      "Loss: 359.6328979492188\n",
      "Loss: 365.241064453125\n",
      "Loss: 362.87915649414066\n",
      "Loss: 368.33392944335935\n",
      "Loss: 364.3289001464844\n",
      "Loss: 370.9774291992187\n",
      "Loss: 365.21134643554683\n",
      "Loss: 366.7885559082031\n",
      "Loss: 362.32252807617186\n",
      "Loss: 367.5102844238281\n",
      "Loss: 364.5760681152343\n",
      "Loss: 365.22443847656245\n",
      "Loss: 361.5390991210937\n",
      "Loss: 365.6914978027344\n",
      "Loss: 363.94951171874993\n",
      "Loss: 363.44040527343753\n",
      "Loss: 364.1939758300781\n",
      "Loss: 363.43438110351565\n",
      "Loss: 360.1078186035156\n",
      "Loss: 361.16740722656255\n",
      "Loss: 369.4418579101563\n",
      "Loss: 363.2670593261719\n",
      "Loss: 366.82037963867185\n",
      "Loss: 364.3878601074219\n",
      "Loss: 366.23602905273435\n",
      "Loss: 364.7438110351562\n",
      "Loss: 368.6345520019531\n",
      "Loss: 366.6987365722656\n",
      "Loss: 362.488818359375\n",
      "Loss: 363.5907958984375\n",
      "Loss: 366.9233703613281\n",
      "Loss: 373.84722900390625\n",
      "Loss: 369.1381530761719\n",
      "Loss: 368.88287353515625\n",
      "Loss: 371.420849609375\n",
      "Loss: 369.6180908203125\n",
      "Loss: 363.39320068359376\n",
      "Loss: 365.99904785156247\n",
      "Loss: 367.86834716796875\n",
      "Loss: 370.24588012695307\n",
      "Loss: 367.305712890625\n",
      "Loss: 368.913720703125\n",
      "Loss: 370.4788879394531\n",
      "Loss: 369.9518737792968\n",
      "Loss: 368.78594970703125\n",
      "Loss: 373.4052429199219\n",
      "Loss: 376.55816040039065\n",
      "Loss: 370.6935668945312\n",
      "Loss: 367.3841491699219\n",
      "Loss: 369.9161071777344\n",
      "Loss: 368.2654235839844\n",
      "Loss: 371.05955200195314\n",
      "Loss: 366.7302612304688\n",
      "Loss: 367.8968933105469\n",
      "Loss: 372.1447937011719\n",
      "Loss: 367.21119384765626\n",
      "Loss: 371.8003784179687\n",
      "Loss: 368.0581298828125\n",
      "Loss: 366.9651611328125\n",
      "Loss: 368.3560607910156\n",
      "Loss: 370.56620483398444\n",
      "Loss: 368.37958984375\n",
      "Loss: 367.95928955078125\n",
      "Loss: 372.43979492187503\n",
      "Loss: 372.54343872070314\n",
      "Loss: 366.5681884765625\n",
      "Loss: 370.56329345703125\n",
      "Loss: 372.993310546875\n",
      "Loss: 369.6599609375\n",
      "Loss: 378.7369689941406\n",
      "Loss: 378.8612426757812\n",
      "Loss: 369.78585815429693\n",
      "Loss: 375.5705200195313\n",
      "Loss: 368.6902099609375\n",
      "Loss: 366.9383605957031\n",
      "Loss: 372.3356079101562\n",
      "Loss: 365.31500244140625\n",
      "Loss: 367.1286254882813\n",
      "Loss: 368.3236328125\n",
      "Loss: 370.7656555175781\n",
      "Loss: 363.55168457031243\n",
      "Loss: 365.4045959472656\n",
      "Loss: 364.2699096679687\n",
      "Loss: 371.7354370117188\n",
      "Loss: 366.05195922851556\n",
      "Loss: 366.29542846679686\n",
      "Loss: 367.7187316894531\n",
      "Loss: 368.76698608398436\n",
      "Loss: 368.0068603515625\n",
      "Loss: 373.04420776367186\n",
      "Loss: 359.5689270019531\n",
      "Loss: 369.5476318359375\n",
      "Loss: 364.5223266601563\n",
      "Loss: 365.1598449707031\n",
      "Loss: 372.29420166015626\n",
      "Loss: 366.9287048339844\n",
      "Loss: 363.7834106445313\n",
      "Loss: 361.48718261718756\n",
      "Loss: 366.1432739257813\n",
      "Loss: 364.65651245117186\n",
      "Loss: 360.9236633300781\n",
      "Loss: 358.0458740234375\n",
      "Loss: 357.5817565917968\n",
      "Loss: 363.3791015625\n",
      "Loss: 362.4436889648437\n",
      "Loss: 365.5304992675781\n",
      "Loss: 361.74453124999997\n",
      "Loss: 363.6041564941406\n",
      "Loss: 367.45293579101565\n",
      "Loss: 366.40949096679685\n",
      "Loss: 364.73081665039064\n",
      "Loss: 362.6441223144531\n",
      "Loss: 361.9546813964844\n",
      "Loss: 360.18466796875\n",
      "Loss: 372.84312133789064\n",
      "Loss: 365.03148193359374\n",
      "Loss: 363.2009033203125\n",
      "Loss: 364.5382446289063\n",
      "Loss: 364.09665527343753\n",
      "Loss: 361.6835021972656\n",
      "Loss: 364.03450317382817\n",
      "Loss: 364.7872314453125\n",
      "Loss: 368.36245117187497\n",
      "Loss: 368.809619140625\n",
      "Loss: 368.6388427734375\n",
      "Loss: 365.99835205078125\n",
      "Loss: 368.45416259765625\n",
      "Loss: 366.60419921874995\n",
      "Loss: 366.9575134277344\n",
      "Loss: 369.56324462890626\n",
      "Loss: 368.64527587890626\n",
      "Loss: 363.92819824218753\n",
      "Loss: 374.9761840820313\n",
      "Loss: 366.0399719238281\n",
      "Loss: 365.4394104003906\n",
      "Loss: 369.3947570800781\n",
      "Loss: 367.7664733886719\n",
      "Loss: 371.8331176757813\n",
      "Loss: 369.1089233398437\n",
      "Loss: 366.6355163574219\n",
      "Loss: 376.5110717773437\n",
      "Loss: 372.0703491210937\n",
      "Loss: 368.55189208984376\n",
      "Loss: 369.01024780273434\n",
      "Loss: 368.3397338867187\n",
      "Loss: 365.8682861328125\n",
      "Loss: 365.09661254882815\n",
      "Loss: 364.85441284179683\n",
      "Loss: 372.6488952636719\n",
      "Loss: 368.36150512695315\n",
      "Loss: 369.5092224121094\n",
      "Loss: 369.30254516601565\n",
      "Loss: 372.8253234863281\n",
      "Loss: 372.5745788574219\n",
      "Loss: 375.82561645507815\n",
      "Loss: 371.3945373535156\n",
      "Loss: 363.85689086914067\n",
      "Loss: 372.4512390136719\n",
      "Loss: 381.0658813476563\n",
      "Loss: 376.1632202148437\n",
      "Loss: 371.75661621093747\n",
      "Loss: 379.52685546875\n",
      "Loss: 374.42333984375\n",
      "Loss: 378.3099853515625\n",
      "Loss: 371.0712524414062\n",
      "Loss: 373.6430603027344\n",
      "Loss: 378.84113159179685\n",
      "Loss: 365.2352600097656\n",
      "Loss: 373.3510498046875\n",
      "Loss: 373.89439086914064\n",
      "Loss: 369.1196350097656\n",
      "Loss: 370.4837646484375\n",
      "Loss: 372.1220397949219\n",
      "Loss: 373.15083618164056\n",
      "Loss: 373.8792907714844\n",
      "Loss: 373.4712768554687\n"
     ]
    }
   ],
   "source": [
    "before_online_results, after_online_results = test_forgetting_odadil(Xs, ys, Xt, yt, Xt_test, yt_test, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'r': [0.9441666666666667, 0.7175],\n",
       "  'r_ot': [0.9666666666666666, 0.9508333333333334]},\n",
       " 'rbf': {'r': [0.885, 0.77], 'r_ot': [0.9870833333333333, 0.9862499999999998]},\n",
       " 'RF': {'r': [0.88375, 0.58625],\n",
       "  'r_ot': [0.9704166666666667, 0.9573333333333334]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_online_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'r': [0.7366666666666667, 0.8470833333333333],\n",
       "  'r_ot': [0.9654166666666667, 0.9541666666666668]},\n",
       " 'rbf': {'r': [0.78125, 0.7979166666666667],\n",
       "  'r_ot': [0.9866666666666666, 0.9862499999999998]},\n",
       " 'RF': {'r': [0.7220833333333333, 0.7029166666666666],\n",
       "  'r_ot': [0.9695416666666666, 0.9577500000000001]}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_online_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991437802803877"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "m = 0\n",
    "for kc in before_online_results.keys():\n",
    "    for kd in before_online_results[kc].keys():\n",
    "        for i in range(len(before_online_results[kc][kd])):\n",
    "            c += 1\n",
    "            m += after_online_results[kc][kd][i]/before_online_results[kc][kd][i]\n",
    "m/c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
