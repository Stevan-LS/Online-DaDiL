{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SL276123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ot\\backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n",
      "C:\\Users\\SL276123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Tests_ODaDiL import test_dadil, test_odadil, test_forgetting_odadil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # noqa: E402\n",
    "import random  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "\n",
    "from pydil.utils.igmm_modif import IGMM\n",
    "\n",
    "from pydil.ipms.ot_ipms import (  # noqa: E402\n",
    "    JointWassersteinDistance\n",
    ")\n",
    "from pydil.dadil.labeled_dictionary_GMM import LabeledDictionaryGMM\n",
    "from pydil.torch_utils.measures import (  # noqa: E402\n",
    "    UnsupervisedDatasetMeasure,\n",
    "    SupervisedDatasetMeasure\n",
    ")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_datasets = []\n",
    "for i in range(1, 11):\n",
    "    dataset = np.load(f'data/toy_non_linear_100d_dataset_{i}.npy')\n",
    "    list_of_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'C'\n",
    "with open(os.path.join('data', 'mlp_fts_256_target_{}.pkl'.format(target)), 'rb') as f:\n",
    "        dataset = pickle.loads(f.read())\n",
    "\n",
    "Xs, ys = [], []\n",
    "d = None\n",
    "keys = list(dataset.keys())\n",
    "for i in range(len(keys)-1):\n",
    "    features = dataset[keys[i]]['Features']\n",
    "    labels = dataset[keys[i]]['Labels'].argmax(dim=1)\n",
    "    domain = i*np.ones((features.shape[0], 1))\n",
    "    Xs.append(features.float())\n",
    "    ys.append(labels.float())\n",
    "    if d is None:\n",
    "        d = domain\n",
    "    else:\n",
    "        d = np.concatenate([d, domain], axis=0)\n",
    "\n",
    "Xt = dataset[target]['fold 0']['Train']['Features'].float()\n",
    "yt = dataset[target]['fold 0']['Train']['Labels'].float().argmax(dim=1)\n",
    "\n",
    "Xt_test = dataset[target]['fold 0']['Test']['Features'].float()\n",
    "yt_test = dataset[target]['fold 0']['Test']['Labels'].float().argmax(dim=1)\n",
    "d = np.concatenate([d, 2*np.ones((Xt.shape[0], 1))], axis=0)\n",
    "\n",
    "n_domains = int(np.max(d)) + 1\n",
    "n_features = Xt.shape[1]\n",
    "n_classes = int(np.max(yt.numpy())) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "batch_size = 200\n",
    "n_atoms = 3\n",
    "n_classes = 10\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dadil(Xs, ys, Xt, yt, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter):\n",
    "    results = {'lin':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'rbf':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'RF':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}}\n",
    "\n",
    "    Q = []\n",
    "    for Xs_k, ys_k in zip(Xs, ys):\n",
    "        Q.append(\n",
    "            SupervisedDatasetMeasure(\n",
    "                features=Xs_k.numpy(),\n",
    "                labels=ys_k.numpy(),\n",
    "                stratify=True,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu'\n",
    "            )\n",
    "        )\n",
    "    Q.append(\n",
    "        UnsupervisedDatasetMeasure(\n",
    "            features=Xt.numpy(),\n",
    "            batch_size=batch_size,\n",
    "            device='cpu'\n",
    "        )\n",
    "    )\n",
    "    criterion = JointWassersteinDistance()\n",
    "    dictionary = LabeledDictionaryGMM(XP=None,\n",
    "                            YP=None,\n",
    "                            A=None,\n",
    "                            n_samples=n_samples,\n",
    "                            n_dim=n_features,\n",
    "                            n_classes=n_classes,\n",
    "                            n_components=n_atoms,\n",
    "                            weight_initialization='uniform',\n",
    "                            n_distributions=len(Q),\n",
    "                            loss_fn=criterion,\n",
    "                            learning_rate_features=1e-1,\n",
    "                            learning_rate_labels=1e-1,\n",
    "                            learning_rate_weights=1e-1,\n",
    "                            reg_e=0.0,\n",
    "                            n_iter_barycenter=10,\n",
    "                            n_iter_sinkhorn=20,\n",
    "                            n_iter_emd=1000000,\n",
    "                            domain_names=None,\n",
    "                            grad_labels=True,\n",
    "                            optimizer_name='Adam',\n",
    "                            balanced_sampling=True,\n",
    "                            sampling_with_replacement=True,\n",
    "                            barycenter_tol=1e-9,\n",
    "                            barycenter_beta=None,\n",
    "                            tensor_dtype=torch.float32,\n",
    "                            track_atoms=False,\n",
    "                            schedule_lr=False)\n",
    "    dictionary.fit(Q,\n",
    "                n_iter_max=n_iter,\n",
    "                batches_per_it=n_samples // batch_size,\n",
    "                verbose=True)\n",
    "    weights = dictionary.A[-1, :].detach()\n",
    "    XP = [XPk.detach().clone() for XPk in dictionary.XP]\n",
    "    YP = [YPk.detach().clone().softmax(dim=-1) for YPk in dictionary.YP]\n",
    "    Xr, Yr = dictionary.reconstruct(weights=weights)\n",
    "\n",
    "    classifiers_e = {'lin': SVC(kernel='linear', probability=True), 'rbf': SVC(kernel='rbf', probability=True), 'RF': RandomForestClassifier()}\n",
    "    classifiers_r = {'lin': SVC(kernel='linear'), 'rbf': SVC(kernel='rbf',), 'RF': RandomForestClassifier()}\n",
    "\n",
    "    \n",
    "    for key in classifiers_e.keys():\n",
    "        # Without DA\n",
    "        clf_wda = classifiers_r[key]\n",
    "        clf_wda.fit(torch.cat(Xs, dim=0),\n",
    "                torch.cat(ys, dim=0))\n",
    "        yp = clf_wda.predict(Xt)\n",
    "        accuracy_wda = accuracy_score(yp, yt)\n",
    "        results[key]['wda'] += accuracy_wda\n",
    "\n",
    "        # DaDiL-E\n",
    "        clf_e = classifiers_e[key]\n",
    "        predictions = []\n",
    "        for XP_k, YP_k in zip(XP, YP):\n",
    "            # Get atom data\n",
    "            XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "            yp_k = YP_k.argmax(dim=1)\n",
    "            clf_e.fit(XP_k, yp_k)\n",
    "            P = clf_e.predict_proba(Xt)\n",
    "            predictions.append(P)\n",
    "        predictions = np.stack(predictions)\n",
    "        # Weights atomic model predictions\n",
    "        yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "        # Compute statistics\n",
    "        accuracy_e = accuracy_score(yt, yp)\n",
    "        results[key]['e'] += accuracy_e\n",
    "\n",
    "        # DaDiL-E with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            predictions = []\n",
    "            for XP_k, YP_k in zip(XP, YP):\n",
    "                # Get atom data\n",
    "                XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "                weights_k = torch.ones(XP_k.shape[0])/XP_k.shape[0]\n",
    "                weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "                C = torch.cdist(XP_k, Xt, p=2) ** 2\n",
    "                ot_plan = ot.emd(weights_k, weights_t, C, numItermax=1000000)\n",
    "                Yt = ot_plan.T @ YP_k\n",
    "                yt_k = Yt.argmax(dim=1)\n",
    "                clf_e.fit(Xt, yt_k)\n",
    "                P = clf_e.predict_proba(Xt)\n",
    "                predictions.append(P)\n",
    "            predictions = np.stack(predictions)\n",
    "            # Weights atomic model predictions\n",
    "            yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "            # Compute statistics\n",
    "            accuracy_e_ot = accuracy_score(yt, yp)\n",
    "            s += accuracy_e_ot\n",
    "        mean_accuracy_e_ot = s/10\n",
    "        results[key]['e_ot'] += mean_accuracy_e_ot\n",
    "\n",
    "        # DaDiL-R\n",
    "        clf_r = classifiers_r[key]\n",
    "        clf_r.fit(Xr, Yr.argmax(dim=1))\n",
    "        yp = clf_r.predict(Xt)\n",
    "        accuracy_r = accuracy_score(yp, yt)\n",
    "        results[key]['r'] += accuracy_r\n",
    "\n",
    "        # DaDiL-R with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            weights_r = torch.ones(Xr.shape[0])/Xr.shape[0]\n",
    "            weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "            C = torch.cdist(Xr, Xt, p=2) ** 2\n",
    "            ot_plan = ot.emd(weights_r, weights_t, C, numItermax=1000000)\n",
    "            Yt = ot_plan.T @ Yr\n",
    "            clf_r.fit(Xt, Yt.argmax(dim=1))\n",
    "            yp = clf_r.predict(Xt)\n",
    "            accuracy_r_ot = accuracy_score(yp, yt)\n",
    "            s += accuracy_r_ot\n",
    "        results[key]['r_ot'] += s/10\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 7264.110546874999\n",
      "It 2/100, Loss: 5393.187207031249\n",
      "It 3/100, Loss: 3963.7130859374997\n",
      "It 4/100, Loss: 3006.333056640625\n",
      "It 5/100, Loss: 2353.006787109375\n",
      "It 6/100, Loss: 1858.0480224609375\n",
      "It 7/100, Loss: 1474.4659423828125\n",
      "It 8/100, Loss: 1184.0206298828125\n",
      "It 9/100, Loss: 959.5632934570312\n",
      "It 10/100, Loss: 786.7889770507812\n",
      "It 11/100, Loss: 645.5423217773439\n",
      "It 12/100, Loss: 542.1340576171875\n",
      "It 13/100, Loss: 462.6333801269531\n",
      "It 14/100, Loss: 399.08770141601565\n",
      "It 15/100, Loss: 360.6198303222656\n",
      "It 16/100, Loss: 332.4620666503906\n",
      "It 17/100, Loss: 303.83948364257816\n",
      "It 18/100, Loss: 277.38637084960936\n",
      "It 19/100, Loss: 262.4359436035156\n",
      "It 20/100, Loss: 246.7506072998047\n",
      "It 21/100, Loss: 230.53716430664062\n",
      "It 22/100, Loss: 223.20847778320314\n",
      "It 23/100, Loss: 238.5696044921875\n",
      "It 24/100, Loss: 211.16941528320314\n",
      "It 25/100, Loss: 204.50079040527345\n",
      "It 26/100, Loss: 196.11647949218752\n",
      "It 27/100, Loss: 198.8087341308594\n",
      "It 28/100, Loss: 185.7472595214844\n",
      "It 29/100, Loss: 171.32837829589843\n",
      "It 30/100, Loss: 165.0344421386719\n",
      "It 31/100, Loss: 148.6091827392578\n",
      "It 32/100, Loss: 122.58639678955078\n",
      "It 33/100, Loss: 111.65651092529298\n",
      "It 34/100, Loss: 91.06301727294922\n",
      "It 35/100, Loss: 77.55059051513672\n",
      "It 36/100, Loss: 76.14759979248046\n",
      "It 37/100, Loss: 67.55389938354492\n",
      "It 38/100, Loss: 61.741409301757805\n",
      "It 39/100, Loss: 56.08207702636719\n",
      "It 40/100, Loss: 54.7087760925293\n",
      "It 41/100, Loss: 57.404265594482425\n",
      "It 42/100, Loss: 56.0457176208496\n",
      "It 43/100, Loss: 57.14529724121094\n",
      "It 44/100, Loss: 54.83945236206055\n",
      "It 45/100, Loss: 48.41753311157227\n",
      "It 46/100, Loss: 60.76486053466796\n",
      "It 47/100, Loss: 48.92690887451172\n",
      "It 48/100, Loss: 50.82981719970703\n",
      "It 49/100, Loss: 47.41871948242188\n",
      "It 50/100, Loss: 50.07455520629883\n",
      "It 51/100, Loss: 51.35091171264648\n",
      "It 52/100, Loss: 54.093909454345706\n",
      "It 53/100, Loss: 52.44983596801758\n",
      "It 54/100, Loss: 44.907752227783206\n",
      "It 55/100, Loss: 47.25951080322265\n",
      "It 56/100, Loss: 47.6248062133789\n",
      "It 57/100, Loss: 47.89846115112305\n",
      "It 58/100, Loss: 48.213172912597656\n",
      "It 59/100, Loss: 46.72821807861328\n",
      "It 60/100, Loss: 56.62550582885741\n",
      "It 61/100, Loss: 47.1639778137207\n",
      "It 62/100, Loss: 43.94220199584961\n",
      "It 63/100, Loss: 39.78746719360352\n",
      "It 64/100, Loss: 44.475249481201175\n",
      "It 65/100, Loss: 43.173946380615234\n",
      "It 66/100, Loss: 48.412133789062494\n",
      "It 67/100, Loss: 48.895467376708986\n",
      "It 68/100, Loss: 48.26667098999023\n",
      "It 69/100, Loss: 43.869371795654295\n",
      "It 70/100, Loss: 50.47705154418945\n",
      "It 71/100, Loss: 47.78097915649414\n",
      "It 72/100, Loss: 45.68003616333007\n",
      "It 73/100, Loss: 43.750358200073244\n",
      "It 74/100, Loss: 45.8799934387207\n",
      "It 75/100, Loss: 40.653261566162115\n",
      "It 76/100, Loss: 47.78318252563477\n",
      "It 77/100, Loss: 44.79884872436523\n",
      "It 78/100, Loss: 45.625376129150396\n",
      "It 79/100, Loss: 42.347356414794916\n",
      "It 80/100, Loss: 42.9624526977539\n",
      "It 81/100, Loss: 42.17476196289063\n",
      "It 82/100, Loss: 49.478675079345706\n",
      "It 83/100, Loss: 47.18883972167969\n",
      "It 84/100, Loss: 45.973218536376955\n",
      "It 85/100, Loss: 40.87133712768555\n",
      "It 86/100, Loss: 40.83882217407227\n",
      "It 87/100, Loss: 41.39458618164062\n",
      "It 88/100, Loss: 39.85334396362305\n",
      "It 89/100, Loss: 44.16181259155273\n",
      "It 90/100, Loss: 38.4291877746582\n",
      "It 91/100, Loss: 46.300377655029294\n",
      "It 92/100, Loss: 43.59132804870605\n",
      "It 93/100, Loss: 51.01986541748047\n",
      "It 94/100, Loss: 42.82198638916015\n",
      "It 95/100, Loss: 42.70963668823242\n",
      "It 96/100, Loss: 39.49060897827148\n",
      "It 97/100, Loss: 55.18205337524414\n",
      "It 98/100, Loss: 41.818355560302734\n",
      "It 99/100, Loss: 45.2047981262207\n",
      "It 100/100, Loss: 49.4260757446289\n"
     ]
    }
   ],
   "source": [
    "results = test_dadil(Xs, ys, Xt, yt, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'wda': 0.7307142857142858,\n",
       "  'e': 0.4180357142857143,\n",
       "  'e_ot': 0.8917142857142858,\n",
       "  'r': 0.9048214285714286,\n",
       "  'r_ot': 0.9305357142857142},\n",
       " 'rbf': {'wda': 0.7217857142857143,\n",
       "  'e': 0.7391071428571429,\n",
       "  'e_ot': 1.0,\n",
       "  'r': 1.0,\n",
       "  'r_ot': 1.0},\n",
       " 'RF': {'wda': 0.6928571428571428,\n",
       "  'e': 0.6258928571428571,\n",
       "  'e_ot': 0.7177499999999999,\n",
       "  'r': 0.9541071428571428,\n",
       "  'r_ot': 0.7744642857142858}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_odadil(Xs, ys, Xt, yt, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter):\n",
    "    results = {'lin':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'rbf':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}, 'RF':{'wda': 0, 'e':0, 'e_ot':0, 'r':0, 'r_ot':0}}\n",
    "    \n",
    "    Q_sources = []\n",
    "    for Xs_k, ys_k in zip(Xs, ys):\n",
    "        Q_sources.append(\n",
    "            SupervisedDatasetMeasure(\n",
    "                features=Xs_k.numpy(),\n",
    "                labels=ys_k.numpy(),\n",
    "                stratify=True,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    criterion = JointWassersteinDistance()\n",
    "\n",
    "    dictionary_sources = LabeledDictionaryGMM(XP=None,\n",
    "                            YP=None,\n",
    "                            A=None,\n",
    "                            n_samples=n_samples,\n",
    "                            n_dim=n_features,\n",
    "                            n_classes=n_classes,\n",
    "                            n_components=n_atoms,\n",
    "                            weight_initialization='uniform',\n",
    "                            n_distributions=len(Q_sources),\n",
    "                            loss_fn=criterion,\n",
    "                            learning_rate_features=1e-1,\n",
    "                            learning_rate_labels=1e-1,\n",
    "                            learning_rate_weights=1e-1,\n",
    "                            reg_e=0.0,\n",
    "                            n_iter_barycenter=10,\n",
    "                            n_iter_sinkhorn=20,\n",
    "                            n_iter_emd=1000000,\n",
    "                            domain_names=None,\n",
    "                            grad_labels=True,\n",
    "                            optimizer_name='Adam',\n",
    "                            balanced_sampling=True,\n",
    "                            sampling_with_replacement=True,\n",
    "                            barycenter_tol=1e-9,\n",
    "                            barycenter_beta=None,\n",
    "                            tensor_dtype=torch.float32,\n",
    "                            track_atoms=False,\n",
    "                            schedule_lr=False)\n",
    "\n",
    "    dictionary_sources.fit(Q_sources,\n",
    "                n_iter_max=n_iter,\n",
    "                batches_per_it=n_samples // batch_size,\n",
    "                verbose=True)\n",
    "\n",
    "    XP_sources = dictionary_sources.XP\n",
    "    YP_sources = dictionary_sources.YP\n",
    "\n",
    "    dictionary_target = LabeledDictionaryGMM(XP=XP_sources,\n",
    "                                    YP=YP_sources,\n",
    "                                    A=None,\n",
    "                                    n_samples=n_samples,\n",
    "                                    n_dim=n_features,\n",
    "                                    n_classes=n_classes,\n",
    "                                    n_components=n_atoms,\n",
    "                                    weight_initialization='uniform',\n",
    "                                    n_distributions=1,\n",
    "                                    loss_fn=criterion,\n",
    "                                    learning_rate_features=0,\n",
    "                                    learning_rate_labels=0,\n",
    "                                    learning_rate_weights=1e-1,\n",
    "                                    reg_e=0.0,\n",
    "                                    n_iter_barycenter=10,\n",
    "                                    n_iter_sinkhorn=20,\n",
    "                                    n_iter_emd=1000000,\n",
    "                                    domain_names=None,\n",
    "                                    grad_labels=True,\n",
    "                                    optimizer_name='Adam',\n",
    "                                    balanced_sampling=True,\n",
    "                                    sampling_with_replacement=True,\n",
    "                                    barycenter_tol=1e-9,\n",
    "                                    barycenter_beta=None,\n",
    "                                    tensor_dtype=torch.float32,\n",
    "                                    track_atoms=False,\n",
    "                                    schedule_lr=False,\n",
    "                                    min_components=10,\n",
    "                                    max_step_components=10,\n",
    "                                    max_components=20)\n",
    "    \n",
    "    n_batch = 20\n",
    "    i = 0\n",
    "    while i < Xt.shape[0]-n_batch:\n",
    "        dictionary_target.fit_target_sample(Xt[i:i+n_batch, :],\n",
    "                                            batches_per_it=n_samples // batch_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                            verbose=True,\n",
    "                                            regularization=False,)\n",
    "        i += n_batch\n",
    "\n",
    "    weights = dictionary_target.A[-1, :].detach()\n",
    "    XP = [XPk.detach().clone() for XPk in dictionary_target.XP]\n",
    "    YP = [YPk.detach().clone().softmax(dim=-1) for YPk in dictionary_target.YP]\n",
    "\n",
    "    Xr, Yr = dictionary_target.reconstruct(weights=weights)\n",
    "\n",
    "    classifiers_e = {'lin': SVC(kernel='linear', probability=True), 'rbf': SVC(kernel='rbf', probability=True), 'RF': RandomForestClassifier()}\n",
    "    classifiers_r = {'lin': SVC(kernel='linear'), 'rbf': SVC(kernel='rbf',), 'RF': RandomForestClassifier()}\n",
    "\n",
    "    for key in classifiers_e.keys():\n",
    "        # Without DA\n",
    "        clf_wda = classifiers_r[key]\n",
    "        clf_wda.fit(torch.cat(Xs, dim=0),\n",
    "                torch.cat(ys, dim=0))\n",
    "        yp = clf_wda.predict(Xt)\n",
    "        accuracy_wda = accuracy_score(yp, yt)\n",
    "        results[key]['wda'] += accuracy_wda\n",
    "\n",
    "        #DaDiL-E\n",
    "        clf_e = classifiers_e[key]\n",
    "        predictions = []\n",
    "        for XP_k, YP_k in zip(XP, YP):\n",
    "            # Get atom data\n",
    "            XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "            yp_k = YP_k.argmax(dim=1)\n",
    "            clf_e.fit(XP_k, yp_k)\n",
    "            P = clf_e.predict_proba(Xt)\n",
    "            predictions.append(P)\n",
    "        predictions = np.stack(predictions)\n",
    "        # Weights atomic model predictions\n",
    "        yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "        # Compute statistics\n",
    "        accuracy_e = accuracy_score(yt, yp)\n",
    "        results[key]['e'] += accuracy_e\n",
    "\n",
    "        #DaDiL-E with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            predictions = []\n",
    "            for XP_k, YP_k in zip(XP, YP):\n",
    "                # Get atom data\n",
    "                XP_k, YP_k = XP_k.data.cpu(), YP_k.data.cpu()\n",
    "                weights_k = torch.ones(XP_k.shape[0])/XP_k.shape[0]\n",
    "                weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "                C = torch.cdist(XP_k, Xt, p=2) ** 2\n",
    "                ot_plan = ot.emd(weights_k, weights_t, C, numItermax=1000000)\n",
    "                Yt = ot_plan.T @ YP_k\n",
    "                yt_k = Yt.argmax(dim=1)\n",
    "                clf_e.fit(Xt, yt_k)\n",
    "                P = clf_e.predict_proba(Xt)\n",
    "                predictions.append(P)\n",
    "            predictions = np.stack(predictions)\n",
    "            # Weights atomic model predictions\n",
    "            yp = np.einsum('i,inj->nj', weights, predictions).argmax(axis=1)\n",
    "            # Compute statistics\n",
    "            accuracy_e_ot = accuracy_score(yt, yp)\n",
    "            s += accuracy_e_ot\n",
    "        results[key]['e_ot'] += s/10\n",
    "\n",
    "        #DaDiL-R\n",
    "        clf_r = classifiers_r[key]\n",
    "        clf_r.fit(Xr, Yr.argmax(dim=1))\n",
    "        yp = clf_r.predict(Xt)\n",
    "        accuracy_r = accuracy_score(yp, yt)\n",
    "        results[key]['r'] += accuracy_r\n",
    "\n",
    "        #DaDiL-R with last optimal transport\n",
    "        s = 0\n",
    "        for _ in range(10):\n",
    "            weights_r = torch.ones(Xr.shape[0])/Xr.shape[0]\n",
    "            weights_t = torch.ones(Xt.shape[0])/Xt.shape[0]\n",
    "            C = torch.cdist(Xr, Xt, p=2) ** 2\n",
    "            ot_plan = ot.emd(weights_r, weights_t, C, numItermax=1000000)\n",
    "            Yt = ot_plan.T @ Yr\n",
    "            clf_r.fit(Xt, Yt.argmax(dim=1))\n",
    "            yp = clf_r.predict(Xt)\n",
    "            accuracy_r_ot = accuracy_score(yp, yt)\n",
    "            s += accuracy_r_ot\n",
    "        results[key]['r_ot'] += s/10\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1/100, Loss: 6234.732324218749\n",
      "It 2/100, Loss: 4638.29609375\n",
      "It 3/100, Loss: 3427.78125\n",
      "It 4/100, Loss: 2587.117431640625\n",
      "It 5/100, Loss: 2061.343920898438\n",
      "It 6/100, Loss: 1649.3478027343751\n",
      "It 7/100, Loss: 1352.8155761718751\n",
      "It 8/100, Loss: 1094.2416259765625\n",
      "It 9/100, Loss: 894.1794189453126\n",
      "It 10/100, Loss: 710.6870239257813\n",
      "It 11/100, Loss: 590.6761108398437\n",
      "It 12/100, Loss: 484.77144165039067\n",
      "It 13/100, Loss: 404.81666870117186\n",
      "It 14/100, Loss: 338.74931640625005\n",
      "It 15/100, Loss: 278.3031311035156\n",
      "It 16/100, Loss: 237.87523498535157\n",
      "It 17/100, Loss: 197.33466796874998\n",
      "It 18/100, Loss: 168.2675109863281\n",
      "It 19/100, Loss: 142.91923217773436\n",
      "It 20/100, Loss: 124.55365753173828\n",
      "It 21/100, Loss: 107.83352355957032\n",
      "It 22/100, Loss: 93.39157104492188\n",
      "It 23/100, Loss: 82.75978393554688\n",
      "It 24/100, Loss: 73.69974060058594\n",
      "It 25/100, Loss: 65.78096313476563\n",
      "It 26/100, Loss: 59.21064147949219\n",
      "It 27/100, Loss: 53.48043365478515\n",
      "It 28/100, Loss: 48.749976348876956\n",
      "It 29/100, Loss: 44.822634887695315\n",
      "It 30/100, Loss: 41.651403808593756\n",
      "It 31/100, Loss: 38.70573196411133\n",
      "It 32/100, Loss: 35.87806701660156\n",
      "It 33/100, Loss: 34.48361434936523\n",
      "It 34/100, Loss: 32.314728546142575\n",
      "It 35/100, Loss: 30.625901412963866\n",
      "It 36/100, Loss: 28.993142700195314\n",
      "It 37/100, Loss: 27.91822052001953\n",
      "It 38/100, Loss: 26.825846481323243\n",
      "It 39/100, Loss: 25.943535995483398\n",
      "It 40/100, Loss: 25.075758743286134\n",
      "It 41/100, Loss: 23.89303970336914\n",
      "It 42/100, Loss: 23.35937385559082\n",
      "It 43/100, Loss: 22.336760711669925\n",
      "It 44/100, Loss: 22.219620513916013\n",
      "It 45/100, Loss: 21.793343734741214\n",
      "It 46/100, Loss: 21.141472625732423\n",
      "It 47/100, Loss: 20.59695167541504\n",
      "It 48/100, Loss: 20.38586807250977\n",
      "It 49/100, Loss: 20.045343017578126\n",
      "It 50/100, Loss: 19.644855880737303\n",
      "It 51/100, Loss: 19.281114196777345\n",
      "It 52/100, Loss: 19.18587532043457\n",
      "It 53/100, Loss: 18.577527236938476\n",
      "It 54/100, Loss: 18.23093376159668\n",
      "It 55/100, Loss: 18.531601333618163\n",
      "It 56/100, Loss: 17.59601745605469\n",
      "It 57/100, Loss: 17.056858825683594\n",
      "It 58/100, Loss: 16.959305191040038\n",
      "It 59/100, Loss: 16.769228363037108\n",
      "It 60/100, Loss: 16.20887489318848\n",
      "It 61/100, Loss: 15.863082122802735\n",
      "It 62/100, Loss: 15.656133651733398\n",
      "It 63/100, Loss: 14.758017349243165\n",
      "It 64/100, Loss: 14.524303627014161\n",
      "It 65/100, Loss: 14.07931308746338\n",
      "It 66/100, Loss: 13.906640434265137\n",
      "It 67/100, Loss: 13.280271148681642\n",
      "It 68/100, Loss: 13.473731231689454\n",
      "It 69/100, Loss: 12.888868141174317\n",
      "It 70/100, Loss: 14.635747909545898\n",
      "It 71/100, Loss: 13.896683883666991\n",
      "It 72/100, Loss: 18.02389907836914\n",
      "It 73/100, Loss: 14.338432884216306\n",
      "It 74/100, Loss: 11.708426856994627\n",
      "It 75/100, Loss: 11.432106971740723\n",
      "It 76/100, Loss: 11.30553207397461\n",
      "It 77/100, Loss: 11.142182731628418\n",
      "It 78/100, Loss: 10.7809362411499\n",
      "It 79/100, Loss: 10.971635246276856\n",
      "It 80/100, Loss: 10.722670364379884\n",
      "It 81/100, Loss: 10.887110900878906\n",
      "It 82/100, Loss: 10.391298103332518\n",
      "It 83/100, Loss: 10.50340232849121\n",
      "It 84/100, Loss: 10.771025466918946\n",
      "It 85/100, Loss: 10.332640266418455\n",
      "It 86/100, Loss: 10.045468711853028\n",
      "It 87/100, Loss: 10.578876876831053\n",
      "It 88/100, Loss: 10.110056114196777\n",
      "It 89/100, Loss: 10.12891273498535\n",
      "It 90/100, Loss: 10.041465187072752\n",
      "It 91/100, Loss: 9.98853168487549\n",
      "It 92/100, Loss: 10.017412567138672\n",
      "It 93/100, Loss: 10.173093223571778\n",
      "It 94/100, Loss: 10.091839027404784\n",
      "It 95/100, Loss: 10.340678215026855\n",
      "It 96/100, Loss: 9.785004997253418\n",
      "It 97/100, Loss: 9.881367111206055\n",
      "It 98/100, Loss: 10.11571273803711\n",
      "It 99/100, Loss: 9.899854469299315\n",
      "It 100/100, Loss: 9.549475288391113\n",
      "Loss: 290.59168701171876\n",
      "Loss: 277.51416625976566\n",
      "Loss: 260.99460754394534\n",
      "Loss: 270.4417419433594\n",
      "Loss: 270.273291015625\n",
      "Loss: 258.8635131835938\n",
      "Loss: 268.9093811035156\n",
      "Loss: 264.8672790527344\n",
      "Loss: 261.1800537109375\n",
      "Loss: 262.85809936523435\n",
      "Loss: 263.5593627929688\n",
      "Loss: 266.65570068359375\n",
      "Loss: 272.7193603515625\n",
      "Loss: 273.22627563476567\n",
      "Loss: 277.21937255859376\n",
      "Loss: 288.82429199218745\n",
      "Loss: 293.9258605957031\n",
      "Loss: 292.31066284179684\n",
      "Loss: 299.64586181640624\n",
      "Loss: 293.41767578125\n",
      "Loss: 298.0672607421875\n",
      "Loss: 301.4404602050781\n",
      "Loss: 304.5405578613281\n",
      "Loss: 302.1112548828125\n",
      "Loss: 310.741552734375\n",
      "Loss: 308.98504638671875\n",
      "Loss: 306.67474365234375\n",
      "Loss: 309.1946228027344\n",
      "Loss: 314.1667785644531\n",
      "Loss: 316.1591491699219\n",
      "Loss: 317.3653503417969\n",
      "Loss: 319.05479736328124\n",
      "Loss: 323.92072143554685\n",
      "Loss: 326.22388916015626\n",
      "Loss: 331.4061096191406\n",
      "Loss: 329.08869628906245\n",
      "Loss: 330.167138671875\n",
      "Loss: 335.6799682617187\n",
      "Loss: 332.08894042968745\n",
      "Loss: 336.0856384277344\n",
      "Loss: 339.3823791503906\n",
      "Loss: 341.1796569824219\n",
      "Loss: 341.9081787109375\n",
      "Loss: 343.2255004882812\n",
      "Loss: 342.69066162109374\n",
      "Loss: 344.5476135253907\n",
      "Loss: 341.1003173828125\n",
      "Loss: 346.4816101074219\n",
      "Loss: 347.58986816406247\n",
      "Loss: 355.367822265625\n",
      "Loss: 348.07805786132815\n",
      "Loss: 357.6316711425781\n",
      "Loss: 347.1201293945312\n",
      "Loss: 348.37754516601564\n",
      "Loss: 346.4252258300781\n",
      "Loss: 356.04407348632816\n",
      "Loss: 359.9713806152344\n",
      "Loss: 353.8021362304687\n",
      "Loss: 355.08628540039064\n",
      "Loss: 355.62224731445315\n",
      "Loss: 351.0006958007813\n",
      "Loss: 357.9862426757812\n",
      "Loss: 355.44707031250005\n",
      "Loss: 357.3355712890625\n",
      "Loss: 353.0636840820313\n",
      "Loss: 353.1000549316406\n",
      "Loss: 362.0064514160156\n",
      "Loss: 355.4119140625\n",
      "Loss: 356.11466674804683\n",
      "Loss: 354.1634460449219\n",
      "Loss: 357.3801391601562\n",
      "Loss: 351.4062255859375\n",
      "Loss: 358.18540039062503\n",
      "Loss: 355.55841064453125\n",
      "Loss: 353.560791015625\n",
      "Loss: 360.3870422363281\n",
      "Loss: 363.01763305664065\n",
      "Loss: 364.1365600585937\n",
      "Loss: 366.26782836914066\n",
      "Loss: 359.76002197265626\n",
      "Loss: 358.3757080078125\n",
      "Loss: 365.97235107421875\n",
      "Loss: 359.9318786621094\n",
      "Loss: 363.7477233886719\n",
      "Loss: 362.1651916503906\n",
      "Loss: 362.5136962890625\n",
      "Loss: 358.710107421875\n",
      "Loss: 360.5992614746094\n",
      "Loss: 360.34119873046876\n",
      "Loss: 358.1211242675781\n",
      "Loss: 363.2306213378906\n",
      "Loss: 364.83074340820315\n",
      "Loss: 361.82257690429685\n",
      "Loss: 359.640478515625\n",
      "Loss: 359.08124999999995\n",
      "Loss: 361.7120727539062\n",
      "Loss: 356.81494750976566\n",
      "Loss: 355.1181396484375\n",
      "Loss: 364.4139099121094\n",
      "Loss: 358.26091308593755\n",
      "Loss: 366.2452575683594\n",
      "Loss: 355.1551818847656\n",
      "Loss: 356.57232666015625\n",
      "Loss: 353.783984375\n",
      "Loss: 358.16595458984375\n",
      "Loss: 356.8541625976562\n",
      "Loss: 356.9136047363281\n",
      "Loss: 358.6971618652343\n",
      "Loss: 363.6479736328125\n",
      "Loss: 361.030859375\n",
      "Loss: 363.7226257324219\n",
      "Loss: 359.86400146484374\n",
      "Loss: 372.29163208007816\n",
      "Loss: 377.00553588867183\n",
      "Loss: 362.998046875\n",
      "Loss: 365.9754272460938\n",
      "Loss: 364.63457641601565\n",
      "Loss: 367.13580322265625\n",
      "Loss: 370.08966064453125\n",
      "Loss: 364.80390624999995\n",
      "Loss: 369.9821228027344\n",
      "Loss: 359.15936279296875\n",
      "Loss: 360.5528259277344\n",
      "Loss: 363.5657958984375\n",
      "Loss: 357.09771728515625\n",
      "Loss: 361.1197509765625\n",
      "Loss: 356.8138122558594\n",
      "Loss: 359.5142639160156\n",
      "Loss: 364.9930114746094\n",
      "Loss: 357.99762573242185\n",
      "Loss: 356.49519653320306\n",
      "Loss: 361.31029052734374\n",
      "Loss: 361.4334716796875\n",
      "Loss: 364.30189208984376\n",
      "Loss: 360.490087890625\n",
      "Loss: 371.5611145019531\n",
      "Loss: 365.93755493164065\n",
      "Loss: 364.8562805175781\n",
      "Loss: 358.4146301269531\n",
      "Loss: 364.47023315429686\n",
      "Loss: 363.2623046875\n",
      "Loss: 370.25225219726565\n",
      "Loss: 363.8592834472656\n",
      "Loss: 367.84733276367183\n",
      "Loss: 371.17304077148435\n",
      "Loss: 369.76953125\n",
      "Loss: 366.2135070800781\n",
      "Loss: 370.8487243652344\n",
      "Loss: 365.01361694335935\n",
      "Loss: 368.52753295898435\n",
      "Loss: 368.11683349609376\n",
      "Loss: 363.47687988281245\n",
      "Loss: 364.4924377441406\n",
      "Loss: 365.0119567871094\n",
      "Loss: 369.9985290527344\n",
      "Loss: 366.0607360839844\n",
      "Loss: 363.3228637695313\n",
      "Loss: 366.83757324218755\n",
      "Loss: 367.4118408203125\n",
      "Loss: 365.5152709960938\n",
      "Loss: 371.93706665039065\n",
      "Loss: 369.1646484375\n",
      "Loss: 366.2468688964844\n",
      "Loss: 370.1885986328125\n",
      "Loss: 369.4751892089843\n",
      "Loss: 367.91516723632816\n",
      "Loss: 371.5067016601563\n",
      "Loss: 371.26012573242184\n",
      "Loss: 361.7423950195312\n",
      "Loss: 363.23519897460943\n",
      "Loss: 363.01307983398436\n",
      "Loss: 366.9278198242188\n",
      "Loss: 363.49046630859374\n",
      "Loss: 361.75726318359375\n",
      "Loss: 358.70822753906253\n",
      "Loss: 365.6570922851563\n",
      "Loss: 371.6723571777344\n",
      "Loss: 364.7221557617188\n",
      "Loss: 365.3243591308594\n",
      "Loss: 366.504833984375\n",
      "Loss: 368.29949951171875\n",
      "Loss: 368.5421203613281\n",
      "Loss: 365.7535705566406\n",
      "Loss: 367.7277526855469\n",
      "Loss: 367.810498046875\n",
      "Loss: 366.0369750976563\n",
      "Loss: 363.1032348632812\n",
      "Loss: 366.24729003906253\n",
      "Loss: 364.4636169433594\n",
      "Loss: 366.96284790039067\n",
      "Loss: 366.49815673828124\n",
      "Loss: 367.03681030273435\n",
      "Loss: 365.94916381835935\n",
      "Loss: 365.5648559570312\n",
      "Loss: 364.84795532226565\n",
      "Loss: 365.16273193359376\n",
      "Loss: 365.1433410644531\n",
      "Loss: 360.0611572265625\n",
      "Loss: 361.1444885253906\n",
      "Loss: 364.7710327148438\n",
      "Loss: 356.23703613281253\n",
      "Loss: 365.0119140625\n",
      "Loss: 364.88583984375003\n",
      "Loss: 365.2272827148438\n",
      "Loss: 362.7744018554688\n",
      "Loss: 360.73742065429684\n",
      "Loss: 362.83391113281255\n",
      "Loss: 361.5530090332031\n",
      "Loss: 367.63144531250003\n",
      "Loss: 367.4278076171875\n",
      "Loss: 362.4150146484375\n",
      "Loss: 368.1262573242188\n",
      "Loss: 362.37217407226564\n",
      "Loss: 365.600341796875\n",
      "Loss: 364.8589782714844\n",
      "Loss: 358.0678955078125\n",
      "Loss: 370.01939697265624\n",
      "Loss: 359.64610595703124\n",
      "Loss: 360.252001953125\n",
      "Loss: 366.42182006835935\n",
      "Loss: 361.9706909179688\n",
      "Loss: 364.4327514648437\n",
      "Loss: 364.66400146484375\n",
      "Loss: 365.8627807617188\n",
      "Loss: 366.35740966796874\n",
      "Loss: 365.589306640625\n",
      "Loss: 364.10457153320317\n",
      "Loss: 362.9228576660156\n",
      "Loss: 376.78209228515624\n",
      "Loss: 367.0864501953125\n",
      "Loss: 366.9006652832031\n",
      "Loss: 367.6332824707031\n",
      "Loss: 364.1752807617188\n",
      "Loss: 364.27116699218743\n",
      "Loss: 369.7901428222656\n",
      "Loss: 367.7244567871094\n",
      "Loss: 365.4634338378906\n",
      "Loss: 369.3757629394532\n",
      "Loss: 362.7033996582031\n",
      "Loss: 365.03528442382816\n",
      "Loss: 367.9759033203125\n",
      "Loss: 360.2691772460938\n",
      "Loss: 363.57055664062494\n",
      "Loss: 363.26905517578126\n",
      "Loss: 363.3166687011718\n",
      "Loss: 364.32265014648436\n",
      "Loss: 375.9670837402344\n",
      "Loss: 370.7992919921875\n",
      "Loss: 371.1351501464843\n",
      "Loss: 368.7093139648438\n",
      "Loss: 373.83702392578124\n",
      "Loss: 366.7939819335937\n",
      "Loss: 370.986865234375\n",
      "Loss: 369.26533813476567\n",
      "Loss: 364.9177917480469\n",
      "Loss: 369.90183715820314\n",
      "Loss: 367.3433837890625\n",
      "Loss: 363.90550537109374\n",
      "Loss: 366.21649169921875\n",
      "Loss: 364.39609985351564\n",
      "Loss: 367.1396545410156\n",
      "Loss: 366.0207153320313\n",
      "Loss: 363.4311584472656\n",
      "Loss: 358.92724609375\n",
      "Loss: 363.6432739257813\n",
      "Loss: 358.7917785644531\n",
      "Loss: 360.9461853027344\n",
      "Loss: 367.3055419921875\n",
      "Loss: 366.0674682617188\n",
      "Loss: 371.5824890136719\n",
      "Loss: 359.95698852539067\n",
      "Loss: 361.1841796875\n",
      "Loss: 360.08742065429686\n",
      "Loss: 363.6932189941406\n",
      "Loss: 355.16789550781255\n",
      "Loss: 359.9991821289063\n",
      "Loss: 367.0748596191406\n",
      "Loss: 359.84624023437505\n",
      "Loss: 357.67677001953126\n"
     ]
    }
   ],
   "source": [
    "results_odadil = test_odadil(Xs, ys, Xt, yt, n_features, n_samples, n_classes, n_atoms, batch_size, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin': {'wda': 0.7307142857142858,\n",
       "  'e': 0.7516071428571428,\n",
       "  'e_ot': 0.9882142857142855,\n",
       "  'r': 0.8239285714285715,\n",
       "  'r_ot': 0.9876785714285712},\n",
       " 'rbf': {'wda': 0.7217857142857143,\n",
       "  'e': 0.7598214285714285,\n",
       "  'e_ot': 1.0,\n",
       "  'r': 0.9175,\n",
       "  'r_ot': 1.0},\n",
       " 'RF': {'wda': 0.6610714285714285,\n",
       "  'e': 0.7153571428571428,\n",
       "  'e_ot': 0.9875714285714287,\n",
       "  'r': 0.7158928571428571,\n",
       "  'r_ot': 0.9846428571428572}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_odadil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
